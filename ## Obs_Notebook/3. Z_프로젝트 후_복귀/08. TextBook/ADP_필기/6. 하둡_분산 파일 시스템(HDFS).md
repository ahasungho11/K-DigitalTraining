***
0. 
- HDFS와 RDBMS의 차이점 : 상대적인 데이터의 용량
- HDFS : 데이터를 분산하여 복제 저장하는 특징
	-> 거대한 용량의 데이터 저장 가능
	-> 이렇게 저장된 데이터를 기반으로 Hive or 스파크 등의 도구로 데이터를 프로세싱할 수 있음 (스파크가 가장 많이 쓰임)
- RDBMS : 하둡 플랫폼에 저장되는 데이터양에 비해 매우 작음

1. 키워드 검색 속도와 직결되는 것 -> Index 구축
- 키워드(단어)마다 페이지나 단어 위치 번호 부여되어 있음
- html파일 안의 태그 번호를 하나하나 부여한 상태
![[Pasted image 20221019093855.png]]

![[Pasted image 20221019093945.png]]
- 분산처리 된 일반 범용 서버에 여러 개로 나눠 관리 -> GFS/맵리듀스 (at google)
![[Pasted image 20221019094237.png]]

2. 하둡의 구성(2component)
- 큰 데이터를 저장 : HDFS(=GFS) 
- 큰 데이터를 처리 : MapReduce
=> 하둡(hadoop) = HDFS(신뢰할 수 있는 공유 storage) + 분산 컴퓨팅(MapReduce)

![[Pasted image 20221019094436.png]]

- ScaleUp : 자원(CPU, 메모리, disk)이 부족하게 되었을 때, 서버의 자원들을 교체하면서 작업을 했어야 함 
- 하둡과 같은 오픈 소스 시스템들이 개발되면서 공요하드웨어와 리눅스를 이용해서 비용감소 大

3. 분산 처리 시스템에서 중요한 point
![[Pasted image 20221019094827.png]]
	i) 데이터를 가진 컴퓨터의 장애 발생시, 다른 컴퓨터나 서버에 문제를 일으키면 안된다는 것
	ii) 자원이 있는 컴퓨터에서 컴퓨팅이 발생해야 한다는 것
	(=> 해당 서버 컴퓨터 자원을 이용해서 컴퓨팅이 발생해야 한다는 것)
	(각 데이터가 들어있는 노드 안의 cpu나 disk로 컴퓨팅이 수행됨(분산처리가 된 결과 조각들을 하나로 모으면 큰 대역폭을 쓰지 않아도 된다는 것))

4. 하둡이 운영되는 3가지 형태
![[Pasted image 20221019095700.png]]
1) 직접 Apache에서 오픈 소스를 받아서 설치해서 운영
2) 업체와 계약을 맺어 유지, 보수를 받으며 운영 (多)
3) 클라우드 이용 (빅데이터 처리 시스템 간편하게 이용)

5. 하둡으로 이용 가능한 서비스
![[Pasted image 20221019105526.png]]
- 하둡뿐만 아니라 같은 생태계에 있는 다른 것들로도 이용 가능

6. 빅데이터 처리 시스템
![[Pasted image 20221019105642.png]]
- 대부분 이런 구조
- 마스터의 역할이 중요(슬레이브 조절 및 제어)
- 슬레이브는 한대가 죽더라도 다른 것은 영향이 없이 돌아가도록 하기 위한 것 (컴퓨팅 실행, 데이터 저장, 자원 관리)

![[Pasted image 20221019105807.png]]
![[Pasted image 20221022023722.png]]
- JobTracker가 TaskTracker의 해당 job이 잘 돌아가는지, task들의 문제가 없는지를 모두다 체크하여 많은 부하를 받음

![[Pasted image 20221019110107.png]]
- 해) YARN으로 리소스 관리 매커니즘으로 변경 (JobTracker -> ResourceManager, TaskTracker -> NodeManager)

![[Pasted image 20221019130217.png]]
- 하둡을 이렇게 3가지 Component로 구분하기도 (HDFS, MapReduce, YARN)

1) 맵리듀스 
- 데이터 처리 작업 정의하는 것으로 생각하면 됨
- 분산 데이터를 어떻게 가져와서, 원하는 결과를 어떻게 취합해서 표현할지 정의하는 framework

 2) 얀(YARN)
	- 응용프로그램을 만들어서 서버로 옮겼을 때, 데이터 처리를 어떻게 실행시킬 것인지
	- job의 분배, 컴퓨터들의 관리, 리소스 파악

3) HDFS
	- 데이터 저장 작업을 관리하는 framework

![[Pasted image 20221019142933.png]]

- 단일 고장 지점(_single point of failure_, _SPOF_)
	: 시스템 구성 요소 중에서, 동작하지 않으면 전체 시스템이 중단되는 요소
	 ex) 네임노드 하나가 문제가 발생하면 데이터노드에 데이터가 있더라도 살릴 방법이 없게됨

- 하둡1.0에서 NameNode의 문제가 발생하게 되면 조치를 취할 수 있는 부분이 많지 않았음 (해결 시간이 많이 걸림)
--> 하둡2.0이 되면서 개선 多 (다른 NameNode가 역할을 할 수 있게됨)

7. 하둡의 동작모드
![[Pasted image 20221019143425.png]]
- 실무에서는 '유사 분산 모드'를 많이 씀
- 현업 운영 환경에서 하둡의 배포를 위한 모드는 '완전 분산 모드'

***
8. HDFS (하둡 분산 파일 시스템)
![[Pasted image 20221019143721.png]]
- Master(NameNode)
	: 데이터 노드들의 위치나 메타 데이터에 대한 관리를 통해 파일 시스템 유지
	: 메타 데이터(데이터의 위치, 속성, 이름, 크기, 전체적 구조 등)들이 저장되는 곳
	(디스크x, 메모리o -> 속도 빠름
	데이터노드 모니터링 -> 3초마다 하트비트(heartbeat)를 전송 (like 생존신고)
	블록관리 -> 장애가 발생한 데이터노드의 블록을 새로운 데이터노드에 복제

- Slave(DataNode)
	: 실제 데이터가 저장되는 곳
	클라이언트가 HDFS에 저장하는 파일을 로컬 디스크에 유지
	실제 저장되는 데이터

![[Pasted image 20221022024303.png]]

![[Pasted image 20221019143921.png]]

![[Pasted image 20221019144149.png]]
- HDFS는 빅데이터를 다루기 때문에, 작은 용량으로 쪼개는 것이 아님
- 큰 데이터들을 빠르게 다루기 위해서, 블록들 하나하나 자체가 큼
- 그림에서 8개의 파일 중에서 하나라도 없으면 오류가 나게되므로, 파일 하나당 3개의 노드를 거쳐 사용하겠다(복제)라고 한다면 => 24개의 블록으로 3개의 서버에 나눠지게 되는 것임

![[Pasted image 20221019144419.png]]
- 원본 소스 파일을 HDFS에 저장할 때, 블록 크기/복제 수를 몇 개로 할 것인가를 설정할 수 있음

![[Pasted image 20221019144545.png]]
- 하둡은 큰 크기의 적은 수의 파일을 다루는데 훨씬 적합!!

9. HDFS 장점
- 분산 프로세싱 지원 - 전체 파일이 아닌 블록
- 고장 제어 - 블록 복제
- 확장성 (Scale Out)
- 비용 효율 (일반적인 하드웨어)
![[Pasted image 20221019145246.png]]

***
10. 데이터센터
![[Pasted image 20221019144805.png]]
- 데이터센터 : 수많은 서버와 컴퓨터들이 렉에 밀집되어 있는 공간
- 하둡을 클러스터로 구성되었다는 가정하의 예시 그림
	- 렉에 각각의 서버들이 마운트, 상단에는 NW장비가 탑재
	- 각각의 렉에 하둡의 서버들이 구축, 클러스터로 구성되어 운영중
	- block -> 데이터노드 각각에 복제되어 저장
	- 현재 로그파일 'daily-01.log' 하나가 3개로 복제되어 가각의 렉이 1개씩 들어가 있는 상황

11. 
![[Pasted image 20221019145350.png]]
- 앞에서 얘기했듯, 1GB의 파일이 들어왔을 때, 128MB씩 8개로 쪼개고
- HDFS를 쓰기 위해, 네임노드에 해당 data를 쓰기위해 메타데이터 작성 시작
- 파일명에 해당되는 블록명과 실제로 저장되는 위치를 <span style="color: #ffd33d">네임노드</span>가 메타데이터 형태로 갖고 있게 됨
	실제로 Big.txt를 부르려고 할 때,  빠르게 분산처리를 읽어오거나 어플을 돌릴 수 있는 것임

![[Pasted image 20221019145922.png]]
- 실제 블록을 저장할 때, 'BLK_일련번호(시퀀스넘버)'를 씀

![[Pasted image 20221019150102.png]]
- 블록들에 대한 정보는 실제 데이터 노드에 맵핑 되어 있을 것임
- 3개로 복제하겠다고 했으니, 각 복제된 파일마다 3개의 위치가 적혀 있음
- 처음에는 메타데이터가 디스크에 쓰여지지만, 곧바로 메모리에 반영되어 빠르게 처리 가능(R/W)
- 데이터 노드 -> 네임 노드 (<span style="color: #ffd33d">block report</span> : ~한 블록이 있음을 통지)
- if) 3개 중 일부 데이터 노드가 die -> 최소 복제 개수 3의 현황이 맞지 않게 됨 -> 2에서 3으로 맞추기 위해, 지속적으로 네임노드가 데이터노드에 복제된 개수를 파악 -> 다른 데이터 노드를 선정해서 부족한 복제 개수 맞춰놓는 역할도 함

***
- 읽음 : 데이터노드는 '병렬'로 한번에 읽음 (한번에 순서대로x)
![[Pasted image 20221019150814.png]]

- 쓰기 : 복제개수(3)에 중점을 맞춰서 쓰게됨
![[Pasted image 20221019152012.png]]
	~한 file을 기록해주셈 -> 블록으로 나누어짐 -> 쓸 수 있는 데이터노드 선정 -> (현재 복제 개수 3) -> 첫/둘째/셋째 순차적으로 각 노드에 기록 -> 최종 네임 노드에 완료되었다고 보고 되어야 완료 ( if not) 다시 수행 )

12. 파일 시스템의 역할
- 퍼미션  및 보안
- 데이터 저장 및 검색 제어
- 스토리지 공간 효율적 관리
- 파일 & 폴더 메타데이터 관리
![[Pasted image 20221019152553.png]]

13.  하둡 분산 파일 시스템 => 논리적인 파일 시스템
![[Pasted image 20221019152709.png]]
- 각각에 하둡을 설치할 때, 서버에 응용프로그램 소스 파일을 받아서 설치하게 됨 ->  리눅스 환경 파일 시스템 위에서 설치
- 실제 외부에서 보면, 추상화된 파일 시스템인 하둡  분산 파일 시스템

- 내부 : 1GB -> 128MB, 8개로 나누어서 블록형태로, 3개씩 복제되어 -> 데이터 노드 내의 특정 디렉토리에 저장
- 외부 : 각각의 데이터 노드 서버x, 추상화된 하둡 파일 시스템

![[Pasted image 20221019153241.png]]
- NFTS의 경우, 4KB보다 작은 파일(2KB라면)을 쓸 때는 효율이 떨어지게 됨 (버려지는 공간이 2KB나 되니까)
- HDFS의 경우, 큰 데이터를 다루는 만큼 남는 공간이 남을 확률이 큼. 따라서 남은 공간은 채우지 않음 (논리적인 파일 시스템이어서 가능)

14. HDFS Block Size
![[Pasted image 20221019153629.png]]
- 저장되는 파일의 용량에 맞춰서, 하둡에서 블록사이즈 정하는 것이 중요

15. 복제 계수 (=Replication Factor)
![[Pasted image 20221019153941.png]]
- dfs.replication =  3
	:  데이터의 블록을 3개로 복제해서 데이터노드에 집어 넣겠다
- 위 그림에서 보듯이, DN에 있는 B5와 B6에 문제가 생겨 소실되었더라도, 네임 노드가 데이터 노드를 찾아서 복제 개수 2->3개로 개수를 맞춰주는 작업을 볼 수 있음 -> 메타데이터에도 업데이트 됨

16. 복제 위치 선택
![[Pasted image 20221019155019.png]]
- Rack끼리는 네트워크의 상단의 스위치로 연결되어 있음
- 따라서, Rack의 서로 다른 Mac의 어떤 것을 쓰기 위해서는 네트워크의 대역폭을 이용할 수 밖에 없음

- Rack 1번 서버가 고장이 났더라도, 2번이나 3번 서버에서 데이터를 쓸 수 있지만, 데이터가 크면 클 수록 더 큰 네트워크 대역폭을 쓸 수 밖에 없음

- 하나의 Rack에 쓰게 되면, 네트워크 대역폭은 발생하지 않지만, 중복은 감소 & 1번이 죽으면 살릴 길이 없게 됨

![[Pasted image 20221019155949.png]]
- 2번 경로로 사용하게 됨 (복제 위치)

17. 백업
1) 백업 위치 설정
	네임 노드를 설치할 때, 백업 위치를 추가 설정하면 최초 메타 데이터가 설치될 때, 추가적으로 입력한 위치에 저장됨

2) 보조 네임노드
	체크 포인팅 작업(edits 작업을 fsimage와 함께 병합)을 진행해서 컴퓨팅을 줄여

18. HDFS
|**HDFS 설계(o)**|**HDFS 설계(x)**|
|:---:|:---:|
|큰 파일을 다루기 위해 (수 백 MB, GB, TB 그 이상)|짧은 지연시간을 가진 데이터 액세스 (ex: OLTP)|
|데이터 액세스 (한번 쓰고 여러 번 읽음)|작은 파일 (많은 작은 파일들은 네임 노드의 메모리를 증가시킴)|
|일반적인 하드웨어|임의적인 파일 수정 (추가만 지원)|
|저장된 데이터는 수정되지 않음(추가만 지원이 됨)||

19. 
![[Pasted image 20221019162424.png]]
- 하둡의 component들을 이해하면 good
	~한 component가 있고, 각각에는 ~할 수 있는 파일이 있겠구나