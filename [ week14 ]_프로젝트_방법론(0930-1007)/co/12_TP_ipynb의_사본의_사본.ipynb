{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Acw189lbBeGe"
   },
   "source": [
    "# 네이버 영화 리뷰 감성 분류하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5Tl_s24BeGi"
   },
   "source": [
    "모델은 고정해두고   \n",
    "(1) 데이터 양을 다르게 하고 (절반/전체)   \n",
    "(2) 전처리를 다르게 해서 (불용어 처리 유무, oov를 전체/일부/최소)  \n",
    "다양한 전처리 단계를 통해 분석량 변화  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OnOoqz4uBeGi"
   },
   "source": [
    "### 1. 네이버 영화 리뷰 데이터에 대한 이해와 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8LHP9jqCBsDE",
    "outputId": "f9639c2e-8fc2-4eeb-f36d-dc7bf6522520"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: konlpy in c:\\users\\ahasu\\appdata\\roaming\\python\\python39\\site-packages (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from konlpy) (1.21.5)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from konlpy) (4.8.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\users\\ahasu\\appdata\\roaming\\python\\python39\\site-packages (from konlpy) (1.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ImEJbszFBeGj"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import re # 정규 표현식 관련 모델\n",
    "from konlpy.tag import Okt #Open Korean Text 형태소 분리 모듈\n",
    "from tqdm import tqdm # 진행 상황을 보여주는 모듈\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ef53ZQU4BeGk"
   },
   "outputs": [],
   "source": [
    "# 한글 깨짐 방지\n",
    "import platform\n",
    "\n",
    "if platform.system() == 'Windows': \n",
    "    path = r'c:\\Windows\\Fonts\\gulim.ttf'\n",
    "elif platform.system() == 'Darwin': # Mac OS\n",
    "    path = r'/System/Library/Fonts/AppleGothic'\n",
    "else:\n",
    "    path = r'/usr/share/fonts/truetype/name/NanumMyeongjo.ttf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82FDnoJ9BeGk"
   },
   "source": [
    "##### [1] 데이터 로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ynAhNjx-BeGl"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/data.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/content/drive/MyDrive/data.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:779\u001b[0m, in \u001b[0;36mread_table\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    764\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    765\u001b[0m     dialect,\n\u001b[0;32m    766\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    775\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    776\u001b[0m )\n\u001b[0;32m    777\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 779\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/data.txt'"
     ]
    }
   ],
   "source": [
    "data = pd.read_table('/content/drive/MyDrive/data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "id": "ab8So4B7BeGl",
    "outputId": "e2469a94-c9c8-4d7f-cddd-d67c34c264bb"
   },
   "outputs": [],
   "source": [
    "data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvc54LiNBeGm"
   },
   "source": [
    "##### [2] 데이터 정제하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2NETrUnBeGn"
   },
   "source": [
    "[2 - 1] 정규식을 통한 한글과 공백을 제외한 문자 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DrvzbU7sBeGn",
    "outputId": "e47b5cea-b214-4918-e838-fdfefa487aa0"
   },
   "outputs": [],
   "source": [
    "data['document'] = data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YDSLtfeBeGo"
   },
   "source": [
    "[2 - 2] 중복 유무 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9PnwhcmHBeGo",
    "outputId": "c3207982-5808-47e2-ca74-1dd734a01580"
   },
   "outputs": [],
   "source": [
    "# document열의 중복여부 확인\n",
    "print('전체 데이터 :', len(data))\n",
    "print('고유값 :', data['document'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cz0qfMXtBeGo"
   },
   "source": [
    "전체 데이터에서 고유값과 차이가 난다는 것은 그만큼의 중복값이 있다는 것이니 제거한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fa3ctOqxBeGp"
   },
   "outputs": [],
   "source": [
    "# 중복 데이터 제거\n",
    "data.drop_duplicates(subset=['document'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtYvnyWyBeGp"
   },
   "source": [
    "[2 - 3] 결측치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uk0thMlkBeGp",
    "outputId": "87e9dad6-ac0a-4240-e7c4-f14d4346c93b"
   },
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "id": "676hDE5WBeGp",
    "outputId": "aa5ace0f-ed5e-4a47-d00a-0d0abc45d5a2"
   },
   "outputs": [],
   "source": [
    "# 결측 데이터 확인\n",
    "data.loc[data.document.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TfB52LJLBeGq"
   },
   "outputs": [],
   "source": [
    "# 결측치 제거\n",
    "data = data.dropna(how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "v65TjrvFBeGq",
    "outputId": "b7144231-f96a-4df7-b605-88c5005396cb"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.bar(['1', '0'], data['label'].value_counts(), color = ['steelblue', 'tomato'])\n",
    "plt.show()\n",
    "\n",
    "print(data.groupby('label').size().reset_index(name = 'count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6e2NX3_QBeGq"
   },
   "source": [
    "거의 같은 데이터 자료로 구성되어 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0e3isPI2BeGr"
   },
   "source": [
    "##### [3] 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cEXBW1X3BeGr"
   },
   "source": [
    "[3 - 1] 불용어 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRbMqJOtBeGr"
   },
   "source": [
    "https://www.ranks.nl/stopwords/korean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Wl6che-BeGr"
   },
   "source": [
    "참고) 기본적인 한국어 불용어 리스트 100개\n",
    "- https://bab2min.tistory.com/544\n",
    "    * 단, 이 리스트는 구어나 인터넷의 가벼운 글들을 반영하지 않아서 블로그/SNS 등에는 적합하지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIduTQEfBeGr"
   },
   "source": [
    "**불용어 지정 - 분기 (1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rgPUt5CpBeGr"
   },
   "outputs": [],
   "source": [
    "with open(\"/content/drive/MyDrive/stopwords.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    stopwords = f.readlines()\n",
    "    stopwords = stopwords[0].split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ziHdmxP1BeGs",
    "outputId": "4c3f149c-43c8-4e2a-9bf6-8112d510a87b"
   },
   "outputs": [],
   "source": [
    "print(len(stopwords))\n",
    "print(stopwords[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bob_dhNPBeGs"
   },
   "source": [
    "889개의 불용어로 구성되어 있는데 조금 지나치게 많은거 아닌가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VcQIJQvABeGs"
   },
   "outputs": [],
   "source": [
    "stopwords_01 = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WtWkqGPdBeGs"
   },
   "outputs": [],
   "source": [
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0TA4Gjc2BeGs",
    "outputId": "0fcbf1ab-64f7-4594-94d1-01819ea8e0bc"
   },
   "outputs": [],
   "source": [
    "# 토큰화만 형태소로 진행된 경우\n",
    "X_train_00 = []\n",
    "# 토큰화와 불용어 적게 제거한 경우\n",
    "X_train_01 = []\n",
    "# 토큰화와 형태소, 불용어 많이 제거한 경우\n",
    "X_train_02 = []\n",
    "\n",
    "# 00\n",
    "for sentence in tqdm(data['document']):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    X_train_00.append(tokenized_sentence)\n",
    "\n",
    "# 01\n",
    "for sentence in tqdm(data['document']):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords_01] # 불용어 적게 제거\n",
    "    X_train_01.append(stopwords_removed_sentence)\n",
    "\n",
    "# 02\n",
    "for sentence in tqdm(data['document']):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 많이 제거\n",
    "    X_train_02.append(stopwords_removed_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mt02RygiBeGt"
   },
   "source": [
    "##### [4] 정수 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "APVLXLomBeGt"
   },
   "outputs": [],
   "source": [
    "# 각각의 데이터에 대한 Tokenizer 생성\n",
    "tokenizer_0 = Tokenizer()\n",
    "tokenizer_1 = Tokenizer()\n",
    "tokenizer_2 = Tokenizer()\n",
    "# 각각의 Tokenizer 훈련\n",
    "tokenizer_0.fit_on_texts(X_train_00)\n",
    "tokenizer_1.fit_on_texts(X_train_01)\n",
    "tokenizer_2.fit_on_texts(X_train_02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hHfFgZyqBeGt",
    "outputId": "1e0c1645-ac7f-4797-dda2-4bf04dbdcd91"
   },
   "outputs": [],
   "source": [
    "print('불용어를 제거하지 않는 경우  :', len(tokenizer_0.word_index))\n",
    "print('불용어를 일부만 제거한 경우  :', len(tokenizer_1.word_index))\n",
    "print('불용어를 많이 제거한 경우    :', len(tokenizer_2.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yK3m8ykKBeGu"
   },
   "outputs": [],
   "source": [
    "def checkToken(tokenizer, threshold) :\n",
    "    total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "    rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "    total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "    rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "    # 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "    for key, value in tokenizer.word_counts.items():\n",
    "        total_freq = total_freq + value\n",
    "\n",
    "        # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "        if(value < threshold):\n",
    "            rare_cnt = rare_cnt + 1\n",
    "            rare_freq = rare_freq + value\n",
    "    \n",
    "\n",
    "    total = (rare_freq / total_freq)*100\n",
    "\n",
    "    print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "    print('등장 빈도가 %s번 미만인 희귀 단어의 수: %s'%(threshold, rare_cnt))\n",
    "    print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "    print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n",
    "    return (rare_freq / total_freq) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJ7Es-CEfc3J"
   },
   "source": [
    "**분기점 2 - 희귀 단어 수 설정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YEzf05AQBeGu",
    "outputId": "0b8306e9-24f3-4554-dc22-0ec858ca2cc0"
   },
   "outputs": [],
   "source": [
    "num_pro_0_30 = checkToken(tokenizer_0, 30)\n",
    "print('------------------------------')\n",
    "num_pro_0_3 = checkToken(tokenizer_0, 3)\n",
    "print('------------------------------')\n",
    "num_pro_1_30 = checkToken(tokenizer_1, 30)\n",
    "print('------------------------------')\n",
    "num_pro_1_3 = checkToken(tokenizer_1, 3)\n",
    "print('------------------------------')\n",
    "num_pro_2_30 = checkToken(tokenizer_2, 30)\n",
    "print('------------------------------')\n",
    "num_pro_2_3 = checkToken(tokenizer_2, 3)\n",
    "\n",
    "list_pro_30 = []\n",
    "list_pro_3 = []\n",
    "\n",
    "list_pro_30.append(num_pro_0_30)\n",
    "list_pro_30.append(num_pro_1_30)\n",
    "list_pro_30.append(num_pro_2_30)\n",
    "list_pro_3.append(num_pro_0_3)\n",
    "list_pro_3.append(num_pro_1_3)\n",
    "list_pro_3.append(num_pro_2_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "N2_08DzJBeGu",
    "outputId": "b871df24-f5ac-4c75-9c58-163066c9420f"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24,10))\n",
    "plt.subplot(131)\n",
    "plt.bar(['Stopword X', 'Stopword S', 'Stopword L'], list_pro_30)\n",
    "plt.ylim(0,13)\n",
    "plt.subplot(132)\n",
    "plt.bar(['Stopword X', 'Stopword S', 'Stopword L'], list_pro_3, color = 'tomato')\n",
    "plt.ylim(0,13)\n",
    "plt.subplot(133)\n",
    "plt.bar(['Stopword X', 'Stopword S', 'Stopword L'], list_pro_3, color = 'tomato')\n",
    "plt.ylim(1,2.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nCC3aAkXBeGu",
    "outputId": "ebb05179-1695-4ab0-9a67-206be521097c"
   },
   "outputs": [],
   "source": [
    "total_cnt_0 = 43770\n",
    "total_cnt_1 = 43752\n",
    "total_cnt_2 = 43431\n",
    "\n",
    "rare_cnt_0_1  = 39276\n",
    "rare_cnt_0_2  = 24340\n",
    "\n",
    "rare_cnt_1_1  = 39189\n",
    "rare_cnt_1_2  = 24307\n",
    "\n",
    "vocab_size_0_00 = total_cnt_0 + 1\n",
    "vocab_size_0_03 = total_cnt_0 - rare_cnt_0_1 + 1\n",
    "vocab_size_0_30 = total_cnt_0 - rare_cnt_0_2 + 1\n",
    "\n",
    "vocab_size_1_00 = total_cnt_1 + 1\n",
    "vocab_size_1_03 = total_cnt_1 - rare_cnt_0_1 + 1\n",
    "vocab_size_1_30 = total_cnt_1 - rare_cnt_0_2 + 1\n",
    "\n",
    "vocab_size_2_00 = total_cnt_2 + 1\n",
    "vocab_size_2_03 = total_cnt_2 - rare_cnt_1_1 + 1\n",
    "vocab_size_2_30 = total_cnt_2 - rare_cnt_1_2 + 1\n",
    "\n",
    "print(vocab_size_0_00)\n",
    "print(vocab_size_0_03)\n",
    "print(vocab_size_0_30)\n",
    "print(vocab_size_1_00)\n",
    "print(vocab_size_1_03)\n",
    "print(vocab_size_1_30)\n",
    "print(vocab_size_2_00)\n",
    "print(vocab_size_2_03)\n",
    "print(vocab_size_2_30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6PbD33xBeGv"
   },
   "source": [
    "텍스트 시퀀스를 정수 시퀀스로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fv660MWoBeGv"
   },
   "outputs": [],
   "source": [
    "tokenizer_0_0 = Tokenizer(vocab_size_0_00)\n",
    "tokenizer_0_3 = Tokenizer(vocab_size_0_03)\n",
    "tokenizer_0_30 = Tokenizer(vocab_size_0_30)\n",
    "tokenizer_1_0 = Tokenizer(vocab_size_1_00)\n",
    "tokenizer_1_3 = Tokenizer(vocab_size_1_03)\n",
    "tokenizer_1_30 = Tokenizer(vocab_size_1_30)\n",
    "tokenizer_1_0 = Tokenizer(vocab_size_2_00)\n",
    "tokenizer_1_3 = Tokenizer(vocab_size_2_03)\n",
    "tokenizer_1_30 = Tokenizer(vocab_size_2_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_M_3q9wBeGv"
   },
   "outputs": [],
   "source": [
    "X_train_0_00 = tokenizer_0_0.fit_on_texts(X_train_00)\n",
    "X_train_0_03 = tokenizer_0_3.fit_on_texts(X_train_00)\n",
    "X_train_0_30 = tokenizer_0_30.fit_on_texts(X_train_00)\n",
    "X_train_1_00 = tokenizer_1_0.fit_on_texts(X_train_01)\n",
    "X_train_1_03 = tokenizer_1_3.fit_on_texts(X_train_01)\n",
    "X_train_1_30 = tokenizer_1_30.fit_on_texts(X_train_01)\n",
    "X_train_2_00 = tokenizer_1_0.fit_on_texts(X_train_02)\n",
    "X_train_2_03 = tokenizer_1_3.fit_on_texts(X_train_02)\n",
    "X_train_2_30 = tokenizer_1_30.fit_on_texts(X_train_02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZcL0Srs6BeGv"
   },
   "outputs": [],
   "source": [
    "X_train_0_00 = tokenizer_0_0.texts_to_sequences(X_train_00)\n",
    "X_train_0_03 = tokenizer_0_3.texts_to_sequences(X_train_00)\n",
    "X_train_0_30 = tokenizer_0_30.texts_to_sequences(X_train_00)\n",
    "X_train_1_00 = tokenizer_1_0.texts_to_sequences(X_train_01)\n",
    "X_train_1_03 = tokenizer_1_3.texts_to_sequences(X_train_01)\n",
    "X_train_1_30 = tokenizer_1_30.texts_to_sequences(X_train_01)\n",
    "X_train_2_00 = tokenizer_1_0.texts_to_sequences(X_train_02)\n",
    "X_train_2_03 = tokenizer_1_3.texts_to_sequences(X_train_02)\n",
    "X_train_2_30 = tokenizer_1_30.texts_to_sequences(X_train_02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TgMc2P9cDzgz"
   },
   "outputs": [],
   "source": [
    "y_train = np.array(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ICrKkMtYFLe6",
    "outputId": "6b1c9884-3d69-4fd2-a660-a10315ee76b1"
   },
   "outputs": [],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpHsXvlvFSJR"
   },
   "source": [
    "[5] 빈 샘플 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eErUQsXYFV4C"
   },
   "outputs": [],
   "source": [
    "drop_train_0_00 = [index for index, sentence in enumerate(X_train_0_00) if len(sentence) < 1]\n",
    "drop_train_0_03 = [index for index, sentence in enumerate(X_train_0_03) if len(sentence) < 1]\n",
    "drop_train_0_30 = [index for index, sentence in enumerate(X_train_0_30) if len(sentence) < 1]\n",
    "\n",
    "drop_train_1_00 = [index for index, sentence in enumerate(X_train_1_00) if len(sentence) < 1]\n",
    "drop_train_1_03 = [index for index, sentence in enumerate(X_train_1_03) if len(sentence) < 1]\n",
    "drop_train_1_30 = [index for index, sentence in enumerate(X_train_1_30) if len(sentence) < 1]\n",
    "\n",
    "drop_train_2_00 = [index for index, sentence in enumerate(X_train_2_00) if len(sentence) < 1]\n",
    "drop_train_2_03 = [index for index, sentence in enumerate(X_train_2_03) if len(sentence) < 1]\n",
    "drop_train_2_30 = [index for index, sentence in enumerate(X_train_2_30) if len(sentence) < 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nOc2qINyFWRq",
    "outputId": "cc172867-9a71-4669-9ac7-534d44a590f7"
   },
   "outputs": [],
   "source": [
    "# 빈 샘플들을 제거\n",
    "X_train_0_00 = np.delete(X_train_0_00, drop_train_0_00, axis=0)\n",
    "X_train_0_03 = np.delete(X_train_0_03, drop_train_0_03, axis=0)\n",
    "X_train_0_30 = np.delete(X_train_0_30, drop_train_0_30, axis=0)\n",
    "X_train_1_00 = np.delete(X_train_1_00, drop_train_1_00, axis=0)\n",
    "X_train_1_03 = np.delete(X_train_1_03, drop_train_1_03, axis=0)\n",
    "X_train_1_30 = np.delete(X_train_1_30, drop_train_1_30, axis=0)\n",
    "X_train_2_00 = np.delete(X_train_2_00, drop_train_2_00, axis=0)\n",
    "X_train_2_03 = np.delete(X_train_2_03, drop_train_2_03, axis=0)\n",
    "X_train_2_30 = np.delete(X_train_2_30, drop_train_2_30, axis=0)\n",
    "\n",
    "print(len(X_train_0_00))\n",
    "print(len(X_train_0_03))\n",
    "print(len(X_train_0_30))\n",
    "print(len(X_train_1_00))\n",
    "print(len(X_train_1_03))\n",
    "print(len(X_train_1_30))\n",
    "print(len(X_train_2_00))\n",
    "print(len(X_train_2_03))\n",
    "print(len(X_train_2_30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hMFiktSKpEPC",
    "outputId": "cb1389dc-5033-419d-ad32-e51d5a0bae94"
   },
   "outputs": [],
   "source": [
    "y_train_0_00 = np.delete(y_train, drop_train_0_00, axis=0)\n",
    "y_train_0_03 = np.delete(y_train, drop_train_0_03, axis=0)\n",
    "y_train_0_30 = np.delete(y_train, drop_train_0_30, axis=0)\n",
    "y_train_1_00 = np.delete(y_train, drop_train_1_00, axis=0)\n",
    "y_train_1_03 = np.delete(y_train, drop_train_1_03, axis=0)\n",
    "y_train_1_30 = np.delete(y_train, drop_train_1_30, axis=0)\n",
    "y_train_2_00 = np.delete(y_train, drop_train_2_00, axis=0)\n",
    "y_train_2_03 = np.delete(y_train, drop_train_2_03, axis=0)\n",
    "y_train_2_30 = np.delete(y_train, drop_train_2_30, axis=0)\n",
    "\n",
    "print(len(y_train_0_00))\n",
    "print(len(y_train_0_03))\n",
    "print(len(y_train_0_30))\n",
    "print(len(y_train_1_00))\n",
    "print(len(y_train_1_03))\n",
    "print(len(y_train_1_30))\n",
    "print(len(y_train_2_00))\n",
    "print(len(y_train_2_03))\n",
    "print(len(y_train_2_30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lKsJPhEnoqzH",
    "outputId": "4cdf6fc4-3e17-4a4e-c4d2-bca27f43538b"
   },
   "outputs": [],
   "source": [
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8EBJwk5FVDl"
   },
   "source": [
    "[6] 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hIfcPfQsFwLi"
   },
   "outputs": [],
   "source": [
    "# 데이터의 길이 확인\n",
    "def checkLen(X_data) :\n",
    "  list_len = []\n",
    "  for i in range(0, len(X_data)):\n",
    "      list_len.append(len(X_data[i]))\n",
    "    \n",
    "  print('리뷰의 최대 길이 :', max(list_len))\n",
    "  print('리뷰의 평균 길이 :', np.mean(list_len))\n",
    "  print('리뷰의 중간 값 길이 :', np.median(list_len))\n",
    "  return list_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ZEF7wS8F4KZ",
    "outputId": "a722e2ba-96f0-4324-8d58-1342aab6b00e"
   },
   "outputs": [],
   "source": [
    "list_len_0_00 = checkLen(X_train_0_00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oXrtl5JbF5Ry",
    "outputId": "cd79883c-abab-4645-c7a4-bd3ec2d1ba74"
   },
   "outputs": [],
   "source": [
    "list_len_2_30 = checkLen(X_train_2_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484
    },
    "id": "vCAS_Pj5F6UL",
    "outputId": "0929c598-0d94-4cf3-efea-c9bd634af62e"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24,10))\n",
    "plt.subplot(2,2,1)\n",
    "plt.hist(list_len_0_00)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.boxplot(list_len_0_00)\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.hist(list_len_2_30)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.boxplot(list_len_2_30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vyFUv_0eF7Ac"
   },
   "outputs": [],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "  count = 0\n",
    "  for sentence in nested_list:\n",
    "    if(len(sentence) <= max_len):\n",
    "        count = count + 1\n",
    "  print(nested_list,'전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (count / len(nested_list))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5PYotwFGF9Kq",
    "outputId": "979f73fa-41c9-4d32-9455-ead4456acd2f"
   },
   "outputs": [],
   "source": [
    "below_threshold_len(40, X_train_0_00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ODWKyJXZF-VM",
    "outputId": "1d5ae521-5034-40ff-b999-dd7db7348907"
   },
   "outputs": [],
   "source": [
    "below_threshold_len(40, X_train_2_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8BD-OESbGAFC"
   },
   "outputs": [],
   "source": [
    "max_len = 40\n",
    "\n",
    "X_data_0_00 = pad_sequences(X_train_0_00, maxlen = max_len)\n",
    "X_data_0_03 = pad_sequences(X_train_0_03, maxlen = max_len)\n",
    "X_data_0_30 = pad_sequences(X_train_0_30, maxlen = max_len)\n",
    "X_data_1_00 = pad_sequences(X_train_1_00, maxlen = max_len)\n",
    "X_data_1_03 = pad_sequences(X_train_1_03, maxlen = max_len)\n",
    "X_data_1_30 = pad_sequences(X_train_1_30, maxlen = max_len)\n",
    "X_data_2_00 = pad_sequences(X_train_2_00, maxlen = max_len)\n",
    "X_data_2_03 = pad_sequences(X_train_2_03, maxlen = max_len)\n",
    "X_data_2_30 = pad_sequences(X_train_2_30, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kOLeZbhHGShR"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "hidden_units = 128\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kcwH1y_iGV6Z"
   },
   "outputs": [],
   "source": [
    "def checkModel(X_train, y_train, vocab_size, name):\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(vocab_size, embedding_dim))\n",
    "  model.add(LSTM(hidden_units))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "  \n",
    "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "  name = (f'{name}.h5')\n",
    "  mc = ModelCheckpoint('.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "  history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)\n",
    "  return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EWc81iCwHc2p",
    "outputId": "37d3111d-f538-4485-bdf3-5c611aa169d8"
   },
   "outputs": [],
   "source": [
    "history_0_00 = checkModel(X_data_0_00, y_train_0_00, vocab_size_0_00, 'X_data_0_00')\n",
    "history_0_03 = checkModel(X_data_0_03, y_train_0_03, vocab_size_0_03, 'X_data_0_03')\n",
    "history_0_30 = checkModel(X_data_0_30, y_train_0_30, vocab_size_0_30, 'X_data_0_30')\n",
    "\n",
    "history_1_00 = checkModel(X_data_1_00, y_train_1_00, vocab_size_1_00, 'X_data_1_00')\n",
    "history_1_03 = checkModel(X_data_1_03, y_train_1_03, vocab_size_1_03, 'X_data_1_03')\n",
    "history_1_30 = checkModel(X_data_1_30, y_train_1_30, vocab_size_1_30, 'X_data_1_30')\n",
    "\n",
    "history_2_00 = checkModel(X_data_2_00, y_train_2_00, vocab_size_2_00, 'X_data_2_00')\n",
    "history_2_03 = checkModel(X_data_2_03, y_train_2_03, vocab_size_2_03, 'X_data_2_03')\n",
    "history_2_30 = checkModel(X_data_2_30, y_train_2_30, vocab_size_2_30, 'X_data_2_30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rxd0ZPWUIDRg"
   },
   "outputs": [],
   "source": [
    "df_history = pd.DataFrame([history_0_00.history['val_acc'], history_0_03.history['val_acc'], history_0_30.history['val_acc'], \n",
    "                           history_1_00.history['val_acc'], history_1_03.history['val_acc'], history_1_30.history['val_acc'], \n",
    "                           history_2_00.history['val_acc'], history_2_03.history['val_acc'], history_2_30.history['val_acc'],\n",
    "                           history_0_00.history['acc'], history_0_03.history['acc'], history_0_30.history['acc'], \n",
    "                           history_1_00.history['acc'], history_1_03.history['acc'], history_1_30.history['acc'], \n",
    "                           history_2_00.history['acc'], history_2_03.history['acc'], history_2_30.history['acc']]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lfs0pyM4IZ6J"
   },
   "outputs": [],
   "source": [
    "list_index = ['X_0_00', 'X_0_03', 'X_0_30',\n",
    "              'X_1_00', 'X_1_03', 'X_1_30',\n",
    "              'X_2_00', 'X_2_03', 'X_2_30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9bUwX-MB-giP"
   },
   "outputs": [],
   "source": [
    "list_val_acc = df_history.loc[:,:8]\n",
    "list_acc = df_history.loc[:,9:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lcj9wWwT-lNj"
   },
   "outputs": [],
   "source": [
    "def checkMax(data):\n",
    "  list_acc = []\n",
    "  for i in range(0, len(data.columns)) :\n",
    "    list_acc.append(max(data.iloc[:, i]))\n",
    "  return list_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XK9Ltu-x-mVs"
   },
   "outputs": [],
   "source": [
    "list_v_a = checkMax(list_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frSrn2Pb-nXh"
   },
   "outputs": [],
   "source": [
    "list_a = checkMax(list_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "id": "sL-Hc8vD-ogH",
    "outputId": "373f9bac-a005-4b28-bb21-31b0b455bed9"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24,8))\n",
    "plt.subplot(121)\n",
    "plt.barh(list_index, list_a)\n",
    "plt.xlim(0.83,1)\n",
    "plt.title('Acc')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.barh(list_index, list_v_a)\n",
    "plt.xlim(0.83,0.93)\n",
    "plt.title('Val_Acc')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAiiujBP-tSA"
   },
   "source": [
    "**알 수 있는 사실**  \n",
    "(1) 불용어 제거 : 지나치면 안하느니 못하다.  \n",
    "(2) 낮은 빈도 수 단어 제거 : 적당히 하면 과적합을 줄일 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaaF68VQAbcY"
   },
   "source": [
    "### 예측해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0R_UVFOL_CwE",
    "outputId": "da4e5b91-cd15-4c30-f6c2-dfb1608d10fb"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size_1_03, embedding_dim))\n",
    "model.add(LSTM(hidden_units))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "  \n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_data_1_03, y_train_1_03, epochs=15, callbacks=[es], batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9TqQIJjpADNb"
   },
   "outputs": [],
   "source": [
    "def sentiment_predict(new_sentence):\n",
    "    new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)\n",
    "    new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\n",
    "    new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n",
    "    encoded = tokenizer_1_3.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
    "    pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
    "    score = float(model.predict(pad_new)) # 예측\n",
    "    if(score > 0.5):\n",
    "        print(\"{:.2f}% 확률로 긍정 리뷰입니다.\\n\".format(score * 100))\n",
    "    else:\n",
    "        print(\"{:.2f}% 확률로 부정 리뷰입니다.\\n\".format((1 - score) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vTJaDD7lATdl",
    "outputId": "ff1d9b77-d645-4c7c-ebe6-f2ee8536fb08"
   },
   "outputs": [],
   "source": [
    "# 일반적인 긍정 리뷰\n",
    "sentiment_predict('너무 재밌어요^^')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z5Rq9SHPAgKE",
    "outputId": "c073f0ec-dc2d-4b56-eba4-712e6cf5eb8f"
   },
   "outputs": [],
   "source": [
    "# 일반적인 부정 리뷰\n",
    "sentiment_predict('진짜 심각하네요...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sJnOiRUpAkvj",
    "outputId": "a76aff5a-f6ee-409a-f865-be7fbb138490"
   },
   "outputs": [],
   "source": [
    "# 평론가 긍정 리뷰 : 이동진 - '그랜드 부다페스트 호텔'\n",
    "sentiment_predict('지나온 적 없는 어제의 세계들에 대한 근원적 노스탤지어.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e7b978KsBajD",
    "outputId": "d0e861f2-3b67-49fb-8468-427577dce620"
   },
   "outputs": [],
   "source": [
    "# 평론가 부정 리뷰 : 이동진 - '7광구'\n",
    "sentiment_predict('소재만 있었지, 할 이야기 자체가 없었던 영화.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2TapTzqgCunc",
    "outputId": "7b3257ed-b520-41ab-8f79-862dee495629"
   },
   "outputs": [],
   "source": [
    "# 비꼬기 리뷰\n",
    "sentiment_predict('감독님은 이 영화 덕분에 만수무강하시겠네요 ^^')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "399b942a90945adf822711c817a2148b24a3a269b52160aecaa333816877df5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
