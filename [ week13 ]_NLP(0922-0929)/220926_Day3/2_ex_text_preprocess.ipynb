{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8689def",
   "metadata": {},
   "source": [
    "# 텍스트 전처리\n",
    "----\n",
    "- 패키지 설치\n",
    "    * NLTK : pip install nltk\n",
    "    * KoNLPy : pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "548f3fac",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2021.11.10)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "# NLTK 패키지 설치\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ab525841",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in c:\\programdata\\anaconda3\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from konlpy) (1.21.5)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from konlpy) (1.4.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from konlpy) (4.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75aaeae7",
   "metadata": {},
   "source": [
    "## [1] 토큰화(Tokenization)\n",
    "---\n",
    "- 문장/문서를 의미를 지닌 작은 단위로 나누는 것\n",
    "- 나누어진 단어를 토큰(Token)이라 함\n",
    "- 종류\n",
    "    * 문장 토큰화\n",
    "    * 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6f0d4f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "634714df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c02e4441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK Corpus 말뭉치 데이터셋 다운로드 받기\n",
    "nltk.download('all', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4cfee5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text1=\"hen tokenizing a Unicode string.\\\n",
    "           NLTK tokenizers can produce token-spans.\\\n",
    "           hen tokenizing a Unicode string.\"\n",
    "raw_text2=\"This particular tokenizer requires the Punkt sentence tokenization.\\\n",
    "           which splits text on whitespace and punctuation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "935e7d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 단위 토큰화\n",
    "result1=word_tokenize(raw_text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1fa99915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hen', 'tokenizing', 'a', 'Unicode', 'string', '.', 'NLTK', 'tokenizers', 'can', 'produce', 'token-spans', '.', 'hen', 'tokenizing', 'a', 'Unicode', 'string', '.']\n"
     ]
    }
   ],
   "source": [
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "df7a0de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 단위 토큰화\n",
    "raw_text=[raw_text1, raw_text2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "03a723bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hen tokenizing a Unicode string.           NLTK tokenizers can produce token-spans.           hen tokenizing a Unicode string.',\n",
       " 'This particular tokenizer requires the Punkt sentence tokenization.           which splits text on whitespace and punctuation.']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "701719d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=sent_tokenize(raw_text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d5904721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hen tokenizing a Unicode string.', 'NLTK tokenizers can produce token-spans.', 'hen tokenizing a Unicode string.'] 3\n"
     ]
    }
   ],
   "source": [
    "print(result, len(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d185920",
   "metadata": {},
   "source": [
    "### 여러 문장에 토큰 추출\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f9949bbc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent => ['hen tokenizing a Unicode string.', 'NLTK tokenizers can produce token-spans.', 'hen tokenizing a Unicode string.']\n",
      "ele => hen tokenizing a Unicode string.\n",
      "wordResult => ['hen', 'tokenizing', 'a', 'Unicode', 'string', '.']\n",
      "ele => NLTK tokenizers can produce token-spans.\n",
      "wordResult => ['NLTK', 'tokenizers', 'can', 'produce', 'token-spans', '.']\n",
      "ele => hen tokenizing a Unicode string.\n",
      "wordResult => ['hen', 'tokenizing', 'a', 'Unicode', 'string', '.']\n",
      "sent => ['This particular tokenizer requires the Punkt sentence tokenization.', 'which splits text on whitespace and punctuation.']\n",
      "ele => This particular tokenizer requires the Punkt sentence tokenization.\n",
      "wordResult => ['This', 'particular', 'tokenizer', 'requires', 'the', 'Punkt', 'sentence', 'tokenization', '.']\n",
      "ele => which splits text on whitespace and punctuation.\n",
      "wordResult => ['which', 'splits', 'text', 'on', 'whitespace', 'and', 'punctuation', '.']\n"
     ]
    }
   ],
   "source": [
    "# 문장 단위로 추출\n",
    "for sent in raw_text:\n",
    "    total_token=set()\n",
    "    #문장 추출\n",
    "    sentResult=sent_tokenize(sent)\n",
    "    \n",
    "    # 문장에서 추출한 토큰\n",
    "    print(f'sent => {sentResult}')\n",
    "    \n",
    "    for ele in sentResult:\n",
    "        print(f'ele => {ele}')\n",
    "        wordResult=word_tokenize(ele)\n",
    "        print(f'wordResult => {wordResult}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd7ee0c",
   "metadata": {},
   "source": [
    "#### 한글 \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d585a9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "# 행태소 분리 객체\n",
    "okt=Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f97ee160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오늘', '은', '월요일', '입니다', '.']\n"
     ]
    }
   ],
   "source": [
    "# 형태소 분리\n",
    "result=okt.morphs(\"오늘은 월요일입니다.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a283ac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 행태소 분리 후 태깅(Tagging) => 품사\n",
    "result2=okt.pos(\"오늘은 월요일입니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f9d4e81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('오늘', 'Noun'), ('은', 'Josa'), ('월요일', 'Noun'), ('입니다', 'Adjective'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7056b84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result2=okt.pos(\"오늘은 월요일입니다.\", stem=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6685bcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('오늘', 'Noun'), ('은', 'Josa'), ('월요일', 'Noun'), ('이다', 'Adjective'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5680ec",
   "metadata": {},
   "source": [
    "### [2] 정제 & 정규화\n",
    "---\n",
    "- 불용어 제거 => 노이즈 제거\n",
    "- 텍스트의 동일화 \n",
    "    * 대문자 또는 소문자로 통일\n",
    "    * 문장의 길이"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae79bce5",
   "metadata": {},
   "source": [
    "### [2-1] 불용어 (Stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "df9a306e",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b60cb939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1255c304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_stopwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa23543",
   "metadata": {},
   "source": [
    "### [2-2] 어간 및 표제어 처리\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "96e8cf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a752f1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어간 추출\n",
    "lstem=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4aba96fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('work', 'work', 'work')"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstem.stem('working'), lstem.stem('worked'), lstem.stem('worken')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2b0af3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('happy', 'happy')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstem.stem('happy'), lstem.stem('happiness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c622aeeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('amus', 'amus')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstem.stem('amuse'), lstem.stem('amused')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a0dc2912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표제어(사전에 등록된 단어 추출)\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "afa4503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wlemma=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "bee654fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('work', 'work')"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wlemma.lemmatize('working', 'v'), wlemma.lemmatize('worked', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "91532068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('amuse', 'amuse')"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wlemma.lemmatize('amusing', 'v'), wlemma.lemmatize('amused', 'v')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d16517",
   "metadata": {},
   "source": [
    "### [3] 텍스트 벡터화\n",
    "---\n",
    "- 텍스트 => 수치화\n",
    "- 희소벡터(OHE) : BOW 방식 -->  Count기반, TF-IDF 기반\n",
    "- 밀집벡터 : Embedding 방식 , Word2Vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "29a3d212",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e46a9bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[raw_text1, raw_text2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6601e619",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe=CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a9596aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2349fd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret=ohe.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ebf48145",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t2\n",
      "  (0, 3)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 13)\t2\n",
      "  (0, 17)\t1\n",
      "  (0, 20)\t1\n",
      "  (0, 21)\t2\n",
      "  (0, 22)\t2\n",
      "  (1, 0)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 12)\t1\n",
      "  (1, 14)\t1\n",
      "  (1, 15)\t1\n",
      "  (1, 16)\t1\n",
      "  (1, 18)\t1\n",
      "  (1, 19)\t1\n",
      "  (1, 23)\t1\n",
      "  (1, 24)\t1\n"
     ]
    }
   ],
   "source": [
    "print(type(ret), ret, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c3aa9e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret=ret.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "03a8c1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 25)\n",
      "[[0 1 2 1 0 0 1 0 0 0 0 1 0 2 0 0 0 1 0 0 1 2 2 0 0]\n",
      " [1 0 0 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 0 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(ret.shape, ret, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f8a60ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TF-IDF 기반\n",
    "tfIdf=TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2b958f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_corpus=tfIdf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "bf49efe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse._csr.csr_matrix"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tf_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ca577ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_corpus= tf_corpus.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8c5eee6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.21320072 0.42640143 0.21320072 0.         0.\n",
      "  0.21320072 0.         0.         0.         0.         0.21320072\n",
      "  0.         0.42640143 0.         0.         0.         0.21320072\n",
      "  0.         0.         0.21320072 0.42640143 0.42640143 0.\n",
      "  0.        ]\n",
      " [0.25819889 0.         0.         0.         0.25819889 0.25819889\n",
      "  0.         0.25819889 0.25819889 0.25819889 0.25819889 0.\n",
      "  0.25819889 0.         0.25819889 0.25819889 0.25819889 0.\n",
      "  0.25819889 0.25819889 0.         0.         0.         0.25819889\n",
      "  0.25819889]]\n"
     ]
    }
   ],
   "source": [
    "print(tf_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dadcd4",
   "metadata": {},
   "source": [
    "## 실습 --------------------------------------------------\n",
    "---\n",
    "- 단어 단위 토큰화\n",
    "- 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e8c0b564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#볼용어 추출\n",
    "from nltk import corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "09cde7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords=corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c5632d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts='Wiki is in Ward is original description: The simplest online database that could possibly work.\\\n",
    "Wiki is a piece of server software that allows users to freely create and edit Web page content using any Web browser. Wiki supports hyperlinks and has a simple text syntax for creating new pages and crosslinks between internal pages on the fly.\\\n",
    "Wiki is unusual among group communication mechanisms in that it allows the organization of contributions to be edited in addition to the content itself.Like many simple concepts, \"open editing\" has some profound and subtle effects on Wiki usage. Allowing everyday users to create and edit any page in a Web site is exciting in that it encourages democratic use of the Web and promotes content composition by nontechnical users.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "4a63d1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordTokens=word_tokenize(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "33c76075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132, list)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordTokens), type(wordTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "a2308e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordTokens2 : 85\n"
     ]
    }
   ],
   "source": [
    "# 불용어 제거\n",
    "wordTokens2=[]\n",
    "for word in wordTokens:\n",
    "    if word not in en_stopwords:\n",
    "        wordTokens2.append(word)\n",
    "\n",
    "print(f'wordTokens2 : {len(wordTokens2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "08fd6144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n"
     ]
    }
   ],
   "source": [
    "wordTokens3=[ word for word in wordTokens if word not in en_stopwords ]\n",
    "\n",
    "print(f'wordTokens3 : {len(wordTokens2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cb1bb5",
   "metadata": {},
   "source": [
    "## Tokenizer 객체 생성\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "a89a4b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "6dbc1d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text='Wiki is in Ward is original description: The simplest online database that could possibly work.\\\n",
    "Wiki is a piece of server software that allows users to freely create and edit Web page content using any Web browser. Wiki supports hyperlinks and has a simple text syntax for creating new pages and crosslinks between internal pages on the fly.\\\n",
    "Wiki is unusual among group communication mechanisms in that it allows the organization of contributions to be edited in addition to the content itself.Like many simple concepts, \"open editing\" has some profound and subtle effects on Wiki usage. Allowing everyday users to create and edit any page in a Web site is exciting in that it encourages democratic use of the Web and promotes content composition by nontechnical users.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "13e5258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰으로 나누기\n",
    "tokens=text_to_word_sequence(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "eb32c8f5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 ['wiki', 'is', 'in', 'ward', 'is', 'original', 'description', 'the', 'simplest', 'online', 'database', 'that', 'could', 'possibly', 'work', 'wiki', 'is', 'a', 'piece', 'of', 'server', 'software', 'that', 'allows', 'users', 'to', 'freely', 'create', 'and', 'edit', 'web', 'page', 'content', 'using', 'any', 'web', 'browser', 'wiki', 'supports', 'hyperlinks', 'and', 'has', 'a', 'simple', 'text', 'syntax', 'for', 'creating', 'new', 'pages', 'and', 'crosslinks', 'between', 'internal', 'pages', 'on', 'the', 'fly', 'wiki', 'is', 'unusual', 'among', 'group', 'communication', 'mechanisms', 'in', 'that', 'it', 'allows', 'the', 'organization', 'of', 'contributions', 'to', 'be', 'edited', 'in', 'addition', 'to', 'the', 'content', 'itself', 'like', 'many', 'simple', 'concepts', 'open', 'editing', 'has', 'some', 'profound', 'and', 'subtle', 'effects', 'on', 'wiki', 'usage', 'allowing', 'everyday', 'users', 'to', 'create', 'and', 'edit', 'any', 'page', 'in', 'a', 'web', 'site', 'is', 'exciting', 'in', 'that', 'it', 'encourages', 'democratic', 'use', 'of', 'the', 'web', 'and', 'promotes', 'content', 'composition', 'by', 'nontechnical', 'users']\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens), tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "c5c9934f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wiki', 'is', 'in', 'ward', 'is', 'original', 'description', 'the', 'simplest', 'online', 'database', 'that', 'could', 'possibly', 'work', 'wiki', 'is', 'a', 'piece', 'of', 'server', 'software', 'that', 'allows', 'users', 'to', 'freely', 'create', 'and', 'edit', 'web', 'page', 'content', 'using', 'any', 'web', 'browser', 'wiki', 'supports', 'hyperlinks', 'and', 'has', 'a', 'simple', 'text', 'syntax', 'for', 'creating', 'new', 'pages', 'and', 'crosslinks', 'between', 'internal', 'pages', 'on', 'the', 'fly', 'wiki', 'is', 'unusual', 'among', 'group', 'communication', 'mechanisms', 'in', 'that', 'it', 'allows', 'the', 'organization', 'of', 'contributions', 'to', 'be', 'edited', 'in', 'addition', 'to', 'the', 'content', 'itself', 'like', 'many', 'simple', 'concepts', 'open', 'editing', 'has', 'some', 'profound', 'and', 'subtle', 'effects', 'on', 'wiki', 'usage', 'allowing', 'everyday', 'users', 'to', 'create', 'and', 'edit', 'any', 'page', 'in', 'a', 'web', 'site', 'is', 'exciting', 'in', 'that', 'it', 'encourages', 'democratic', 'use', 'of', 'the', 'web', 'and', 'promotes', 'content', 'composition', 'by', 'nontechnical', 'users']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f24b5d0",
   "metadata": {},
   "source": [
    "### Tokenizer 객체 --------------------------------------------------------------------\n",
    "- 제공한 문서/문장에 대한 단어사전(voca)\n",
    "- 단어사전(voca)에 존재하지 않는 단어 => Out Of Voca : oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "9cd8b9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "  'I love my dog',\n",
    "  'I love my cat',\n",
    "  'You love my dog!',\n",
    "  'Do you think my dog is amazing?'\n",
    "]\n",
    "# {'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, '\n",
    "#  do': 7, 'think': 8, 'is': 9, 'amazing': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "78e7c077",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "# 단어 빈도수가 높은 순으로 낮은 정수 인덱스 부여\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "d709106e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n"
     ]
    }
   ],
   "source": [
    "# 단어 인덱스  : 단어 인덱스\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "533e3abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('i', 2), ('love', 3), ('my', 4), ('dog', 3), ('cat', 1), ('you', 2), ('do', 1), ('think', 1), ('is', 1), ('amazing', 1)])\n"
     ]
    }
   ],
   "source": [
    "# 단어 출력 갯수\n",
    "print(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "9d0a2943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]\n"
     ]
    }
   ],
   "source": [
    "# 문장을 생성된 사전(voca)를 기반으로 수치화 \n",
    "print(tokenizer.texts_to_sequences(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d423250",
   "metadata": {},
   "source": [
    "## One-Hot-Encoding 변환\n",
    "---\n",
    "- sklearn OneHotEncoder객체 생성\n",
    "- kears 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "88a1dce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaa54f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_categorical()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
