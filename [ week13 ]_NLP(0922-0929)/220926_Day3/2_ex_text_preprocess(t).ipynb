{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ea1cbb8",
   "metadata": {},
   "source": [
    "# 텍스트 전처리\n",
    "----\n",
    "- 패키지 설치\n",
    "    * NLTK : pip install nltk\n",
    "    * KoNLPy : pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8cb03a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: click in c:\\users\\ahasu\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "# NLTK 패키지 설치\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e619b35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: konlpy in c:\\users\\ahasu\\appdata\\roaming\\python\\python39\\site-packages (0.6.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from konlpy) (4.8.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\users\\ahasu\\appdata\\roaming\\python\\python39\\site-packages (from konlpy) (1.4.0)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from konlpy) (1.21.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e80be6",
   "metadata": {},
   "source": [
    "## [1] 토큰화(Tokenization)\n",
    "---\n",
    "- 문장/문서를 의미를 지닌 작은 단위로 나누는 것\n",
    "- 나누어진 단어를 토큰(Token)이라 함\n",
    "- 종류\n",
    "    * 문장 토큰화\n",
    "    * 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcafe1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c1a6d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5236e165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK Corpus 말뭉치 데이터셋 다운로드 받기\n",
    "nltk.download('all', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82cf749c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text1=\"hen tokenizing a Unicode string.\\\n",
    "           NLTK tokenizers can produce token-spans.\\\n",
    "           hen tokenizing a Unicode string.\"\n",
    "raw_text2=\"This particular tokenizer requires the Punkt sentence tokenization.\\\n",
    "           which splits text on whitespace and punctuation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5650837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 단위 토큰화\n",
    "result1=word_tokenize(raw_text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cc0cc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hen', 'tokenizing', 'a', 'Unicode', 'string', '.', 'NLTK', 'tokenizers', 'can', 'produce', 'token-spans', '.', 'hen', 'tokenizing', 'a', 'Unicode', 'string', '.']\n"
     ]
    }
   ],
   "source": [
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1e54dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 단위 토큰화\n",
    "raw_text= raw_text1+raw_text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec60f3f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hen tokenizing a Unicode string.           NLTK tokenizers can produce token-spans.           hen tokenizing a Unicode string.This particular tokenizer requires the Punkt sentence tokenization.           which splits text on whitespace and punctuation.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1fc428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=sent_tokenize(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ae8dc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hen tokenizing a Unicode string.', 'NLTK tokenizers can produce token-spans.', 'hen tokenizing a Unicode string.This particular tokenizer requires the Punkt sentence tokenization.', 'which splits text on whitespace and punctuation.'] 4\n"
     ]
    }
   ],
   "source": [
    "print(result, len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "839a695c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hen tokenizing a Unicode string.',\n",
       " 'NLTK tokenizers can produce token-spans.',\n",
       " 'hen tokenizing a Unicode string.This particular tokenizer requires the Punkt sentence tokenization.',\n",
       " 'which splits text on whitespace and punctuation.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67eb3aea",
   "metadata": {},
   "source": [
    "### 여러 문장에 토큰 추출\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d84573d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent => ['hen tokenizing a Unicode string.']\n",
      "ele => hen tokenizing a Unicode string.\n",
      "wordResult => ['hen', 'tokenizing', 'a', 'Unicode', 'string', '.']\n",
      "sent => ['NLTK tokenizers can produce token-spans.']\n",
      "ele => NLTK tokenizers can produce token-spans.\n",
      "wordResult => ['NLTK', 'tokenizers', 'can', 'produce', 'token-spans', '.']\n",
      "sent => ['hen tokenizing a Unicode string.This particular tokenizer requires the Punkt sentence tokenization.']\n",
      "ele => hen tokenizing a Unicode string.This particular tokenizer requires the Punkt sentence tokenization.\n",
      "wordResult => ['hen', 'tokenizing', 'a', 'Unicode', 'string.This', 'particular', 'tokenizer', 'requires', 'the', 'Punkt', 'sentence', 'tokenization', '.']\n",
      "sent => ['which splits text on whitespace and punctuation.']\n",
      "ele => which splits text on whitespace and punctuation.\n",
      "wordResult => ['which', 'splits', 'text', 'on', 'whitespace', 'and', 'punctuation', '.']\n"
     ]
    }
   ],
   "source": [
    "# 문장 단위로 추출\n",
    "for sent in result:\n",
    "    total_token=set()\n",
    "    #문장 추출\n",
    "    sentResult=sent_tokenize(sent)\n",
    "    \n",
    "    # 문장에서 추출한 토큰\n",
    "    print(f'sent => {sentResult}')\n",
    "    \n",
    "    for ele in sentResult:\n",
    "        print(f'ele => {ele}')\n",
    "        wordResult=word_tokenize(ele)\n",
    "        print(f'wordResult => {wordResult}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c949d79f",
   "metadata": {},
   "source": [
    "#### 한글 \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f2d0797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "# 행태소 분리 객체\n",
    "okt=Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b340d5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오늘', '은', '월요일', '입니다', '.']\n"
     ]
    }
   ],
   "source": [
    "# 형태소 분리\n",
    "result=okt.morphs(\"오늘은 월요일입니다.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82e964dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 행태소 분리 후 태깅(Tagging) => 품사\n",
    "result2=okt.pos(\"오늘은 월요일입니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc478f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('오늘', 'Noun'), ('은', 'Josa'), ('월요일', 'Noun'), ('입니다', 'Adjective'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b7f682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result2=okt.pos(\"오늘은 월요일입니다.\", stem=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25d1c1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('오늘', 'Noun'), ('은', 'Josa'), ('월요일', 'Noun'), ('이다', 'Adjective'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d57f8b",
   "metadata": {},
   "source": [
    "### [2] 정제 & 정규화\n",
    "---\n",
    "- 불용어 제거 => 노이즈 제거\n",
    "- 텍스트의 동일화 \n",
    "    * 대문자 또는 소문자로 통일\n",
    "    * 문장의 길이"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c894dd7",
   "metadata": {},
   "source": [
    "### [2-1] 불용어 (Stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "140b15aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "500b553b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "12703e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_stopwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1f7c7b",
   "metadata": {},
   "source": [
    "### [2-2] 어간 및 표제어 처리\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c79d125f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d224ec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어간 추출\n",
    "lstem=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a7d520d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('work', 'work', 'work')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstem.stem('working'), lstem.stem('worked'), lstem.stem('worken')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c41f4d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('happy', 'happy')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstem.stem('happy'), lstem.stem('happiness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6bc53069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('amus', 'amus')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstem.stem('amuse'), lstem.stem('amused')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c721beb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표제어(사전에 등록된 단어 추출)\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "11bfe0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wlemma=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8dce8666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('work', 'work')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wlemma.lemmatize('working', 'v'), wlemma.lemmatize('worked', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eb55370c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('amuse', 'amuse')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wlemma.lemmatize('amusing', 'v'), wlemma.lemmatize('amused', 'v')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da47d7b3",
   "metadata": {},
   "source": [
    "### [3] 텍스트 벡터화\n",
    "---\n",
    "- 텍스트 => 수치화\n",
    "- 희소벡터(OHE) : BOW 방식 -->  Count기반, TF-IDF 기반\n",
    "- 밀집벡터 : Embedding 방식 , Word2Vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e46ed326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5d483577",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[raw_text1, raw_text2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d7b2129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe=CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "24560c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "25c31e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret=ohe.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b64827e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t2\n",
      "  (0, 3)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 13)\t2\n",
      "  (0, 17)\t1\n",
      "  (0, 20)\t1\n",
      "  (0, 21)\t2\n",
      "  (0, 22)\t2\n",
      "  (1, 0)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 12)\t1\n",
      "  (1, 14)\t1\n",
      "  (1, 15)\t1\n",
      "  (1, 16)\t1\n",
      "  (1, 18)\t1\n",
      "  (1, 19)\t1\n",
      "  (1, 23)\t1\n",
      "  (1, 24)\t1\n"
     ]
    }
   ],
   "source": [
    "print(type(ret), ret, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4159707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret=ret.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1139f46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 25)\n",
      "[[0 1 2 1 0 0 1 0 0 0 0 1 0 2 0 0 0 1 0 0 1 2 2 0 0]\n",
      " [1 0 0 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 0 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(ret.shape, ret, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f4eb714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TF-IDF 기반\n",
    "tfIdf=TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a223096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_corpus=tfIdf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a0269f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tf_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ebc9f0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_corpus= tf_corpus.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "01331cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.21320072 0.42640143 0.21320072 0.         0.\n",
      "  0.21320072 0.         0.         0.         0.         0.21320072\n",
      "  0.         0.42640143 0.         0.         0.         0.21320072\n",
      "  0.         0.         0.21320072 0.42640143 0.42640143 0.\n",
      "  0.        ]\n",
      " [0.25819889 0.         0.         0.         0.25819889 0.25819889\n",
      "  0.         0.25819889 0.25819889 0.25819889 0.25819889 0.\n",
      "  0.25819889 0.         0.25819889 0.25819889 0.25819889 0.\n",
      "  0.25819889 0.25819889 0.         0.         0.         0.25819889\n",
      "  0.25819889]]\n"
     ]
    }
   ],
   "source": [
    "print(tf_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2766af7c",
   "metadata": {},
   "source": [
    "## 실습 --------------------------------------------------\n",
    "---\n",
    "- 단어 단위 토큰화\n",
    "- 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bd3320a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#볼용어 추출\n",
    "from nltk import corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8f146b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords=corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e870c009",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts='Wiki is in Ward is original description: The simplest online database that could possibly work.\\\n",
    "Wiki is a piece of server software that allows users to freely create and edit Web page content using any Web browser. Wiki supports hyperlinks and has a simple text syntax for creating new pages and crosslinks between internal pages on the fly.\\\n",
    "Wiki is unusual among group communication mechanisms in that it allows the organization of contributions to be edited in addition to the content itself.Like many simple concepts, \"open editing\" has some profound and subtle effects on Wiki usage. Allowing everyday users to create and edit any page in a Web site is exciting in that it encourages democratic use of the Web and promotes content composition by nontechnical users.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7affc085",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordTokens=word_tokenize(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "08b4440e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132, list)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordTokens), type(wordTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b94ae93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordTokens2 : 85\n"
     ]
    }
   ],
   "source": [
    "# 불용어 제거\n",
    "wordTokens2=[]\n",
    "for word in wordTokens:\n",
    "    if word not in en_stopwords:\n",
    "        wordTokens2.append(word)\n",
    "\n",
    "print(f'wordTokens2 : {len(wordTokens2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "872f5af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordTokens3 : 85\n"
     ]
    }
   ],
   "source": [
    "wordTokens3=[ word for word in wordTokens if word not in en_stopwords ]\n",
    "\n",
    "print(f'wordTokens3 : {len(wordTokens3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fa45098a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wiki',\n",
       " 'Ward',\n",
       " 'original',\n",
       " 'description',\n",
       " ':',\n",
       " 'The',\n",
       " 'simplest',\n",
       " 'online',\n",
       " 'database',\n",
       " 'could',\n",
       " 'possibly',\n",
       " 'work.Wiki',\n",
       " 'piece',\n",
       " 'server',\n",
       " 'software',\n",
       " 'allows',\n",
       " 'users',\n",
       " 'freely',\n",
       " 'create',\n",
       " 'edit',\n",
       " 'Web',\n",
       " 'page',\n",
       " 'content',\n",
       " 'using',\n",
       " 'Web',\n",
       " 'browser',\n",
       " '.',\n",
       " 'Wiki',\n",
       " 'supports',\n",
       " 'hyperlinks',\n",
       " 'simple',\n",
       " 'text',\n",
       " 'syntax',\n",
       " 'creating',\n",
       " 'new',\n",
       " 'pages',\n",
       " 'crosslinks',\n",
       " 'internal',\n",
       " 'pages',\n",
       " 'fly.Wiki',\n",
       " 'unusual',\n",
       " 'among',\n",
       " 'group',\n",
       " 'communication',\n",
       " 'mechanisms',\n",
       " 'allows',\n",
       " 'organization',\n",
       " 'contributions',\n",
       " 'edited',\n",
       " 'addition',\n",
       " 'content',\n",
       " 'itself.Like',\n",
       " 'many',\n",
       " 'simple',\n",
       " 'concepts',\n",
       " ',',\n",
       " '``',\n",
       " 'open',\n",
       " 'editing',\n",
       " \"''\",\n",
       " 'profound',\n",
       " 'subtle',\n",
       " 'effects',\n",
       " 'Wiki',\n",
       " 'usage',\n",
       " '.',\n",
       " 'Allowing',\n",
       " 'everyday',\n",
       " 'users',\n",
       " 'create',\n",
       " 'edit',\n",
       " 'page',\n",
       " 'Web',\n",
       " 'site',\n",
       " 'exciting',\n",
       " 'encourages',\n",
       " 'democratic',\n",
       " 'use',\n",
       " 'Web',\n",
       " 'promotes',\n",
       " 'content',\n",
       " 'composition',\n",
       " 'nontechnical',\n",
       " 'users',\n",
       " '.']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordTokens3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56642e0",
   "metadata": {},
   "source": [
    "## Tokenizer 객체 생성\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "730f9143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "054da984",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text='Wiki is in Ward is original description: The simplest online database that could possibly work.\\\n",
    "Wiki is a piece of server software that allows users to freely create and edit Web page content using any Web browser. Wiki supports hyperlinks and has a simple text syntax for creating new pages and crosslinks between internal pages on the fly.\\\n",
    "Wiki is unusual among group communication mechanisms in that it allows the organization of contributions to be edited in addition to the content itself.Like many simple concepts, \"open editing\" has some profound and subtle effects on Wiki usage. Allowing everyday users to create and edit any page in a Web site is exciting in that it encourages democratic use of the Web and promotes content composition by nontechnical users.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "83ecd8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰으로 나누기\n",
    "tokens=text_to_word_sequence(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "498827a7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 ['wiki', 'is', 'in', 'ward', 'is', 'original', 'description', 'the', 'simplest', 'online', 'database', 'that', 'could', 'possibly', 'work', 'wiki', 'is', 'a', 'piece', 'of', 'server', 'software', 'that', 'allows', 'users', 'to', 'freely', 'create', 'and', 'edit', 'web', 'page', 'content', 'using', 'any', 'web', 'browser', 'wiki', 'supports', 'hyperlinks', 'and', 'has', 'a', 'simple', 'text', 'syntax', 'for', 'creating', 'new', 'pages', 'and', 'crosslinks', 'between', 'internal', 'pages', 'on', 'the', 'fly', 'wiki', 'is', 'unusual', 'among', 'group', 'communication', 'mechanisms', 'in', 'that', 'it', 'allows', 'the', 'organization', 'of', 'contributions', 'to', 'be', 'edited', 'in', 'addition', 'to', 'the', 'content', 'itself', 'like', 'many', 'simple', 'concepts', 'open', 'editing', 'has', 'some', 'profound', 'and', 'subtle', 'effects', 'on', 'wiki', 'usage', 'allowing', 'everyday', 'users', 'to', 'create', 'and', 'edit', 'any', 'page', 'in', 'a', 'web', 'site', 'is', 'exciting', 'in', 'that', 'it', 'encourages', 'democratic', 'use', 'of', 'the', 'web', 'and', 'promotes', 'content', 'composition', 'by', 'nontechnical', 'users']\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens), tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "25395069",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wiki', 'is', 'in', 'ward', 'is', 'original', 'description', 'the', 'simplest', 'online', 'database', 'that', 'could', 'possibly', 'work', 'wiki', 'is', 'a', 'piece', 'of', 'server', 'software', 'that', 'allows', 'users', 'to', 'freely', 'create', 'and', 'edit', 'web', 'page', 'content', 'using', 'any', 'web', 'browser', 'wiki', 'supports', 'hyperlinks', 'and', 'has', 'a', 'simple', 'text', 'syntax', 'for', 'creating', 'new', 'pages', 'and', 'crosslinks', 'between', 'internal', 'pages', 'on', 'the', 'fly', 'wiki', 'is', 'unusual', 'among', 'group', 'communication', 'mechanisms', 'in', 'that', 'it', 'allows', 'the', 'organization', 'of', 'contributions', 'to', 'be', 'edited', 'in', 'addition', 'to', 'the', 'content', 'itself', 'like', 'many', 'simple', 'concepts', 'open', 'editing', 'has', 'some', 'profound', 'and', 'subtle', 'effects', 'on', 'wiki', 'usage', 'allowing', 'everyday', 'users', 'to', 'create', 'and', 'edit', 'any', 'page', 'in', 'a', 'web', 'site', 'is', 'exciting', 'in', 'that', 'it', 'encourages', 'democratic', 'use', 'of', 'the', 'web', 'and', 'promotes', 'content', 'composition', 'by', 'nontechnical', 'users']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e96014",
   "metadata": {},
   "source": [
    "### Tokenizer 객체 --------------------------------------------------------------------\n",
    "- 제공한 문서/문장에 대한 단어사전(voca)\n",
    "- 단어사전(voca)에 존재하지 않는 단어 => Out Of Voca : oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "470f8bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "  'I love my dog',\n",
    "  'I love my cat',\n",
    "  'You love my dog!',\n",
    "  'Do you think my dog is amazing?'\n",
    "]\n",
    "# {'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, '\n",
    "#  do': 7, 'think': 8, 'is': 9, 'amazing': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bfb06559",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token='oov', num_words=5)\n",
    "\n",
    "# 단어 빈도수가 높은 순으로 낮은 정수 인덱스 부여\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c62af847",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'oov': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n"
     ]
    }
   ],
   "source": [
    "# 단어 인덱스  : 단어 인덱스\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "56ebed4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('i', 2), ('love', 3), ('my', 4), ('dog', 3), ('cat', 1), ('you', 2), ('do', 1), ('think', 1), ('is', 1), ('amazing', 1)])\n"
     ]
    }
   ],
   "source": [
    "# 단어 출력 갯수\n",
    "print(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1df3927b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 2, 4, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "# 문장을 생성된 사전(voca)를 기반으로 수치화 \n",
    "print(tokenizer.texts_to_sequences(['We think my dog is amazing?']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52759a9",
   "metadata": {},
   "source": [
    "### 패딩(Padding)\n",
    "---\n",
    "- 길이가 모두 다른 문장들을 동일 길이로 맞추기 위한 과정\n",
    "- 길이 기준 설정\n",
    "- 긴 경우 => 앞/뒤 중 선택\n",
    "- 짧은 경우 =>  앞/뒤 중 선택\n",
    "- 값 => 패딩에 들어갈 값    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0904cbe2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'pad_sequences' from 'tensorflow.keras.utils' (C:\\Users\\ahasu\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\api\\_v2\\keras\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [74]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'pad_sequences' from 'tensorflow.keras.utils' (C:\\Users\\ahasu\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\api\\_v2\\keras\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c240ec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5033423",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422c005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding=pad_sequences(result)\n",
    "encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b8d7f1",
   "metadata": {},
   "source": [
    "## One-Hot-Encoding 변환\n",
    "---\n",
    "- sklearn OneHotEncoder객체 생성\n",
    "- kears 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88497b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212d0669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 생성된 사전(voca)를 기반으로 수치화 \n",
    "seq_voca=tokenizer.texts_to_sequences(sentences)\n",
    "print(f'seq_voca : {len(seq_voca)}')\n",
    "print(seq_voca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877fef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_categorical(seq_voca[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10e459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.texts_to_matrix(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6961943c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0240bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc57cebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer.texts_to_sequences(sentences)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1345bc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded = pad_sequences(encoded)\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c11b050",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded = pad_sequences(encoded, padding='post')\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04a7645",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded = pad_sequences(encoded, padding='post', maxlen=5)\n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c24fd49",
   "metadata": {},
   "source": [
    "## FILE 읽고 벡터화\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f89180d",
   "metadata": {},
   "source": [
    "#### [1] 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da092ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE='../data/example.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49893d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FILE, mode='r') as f:\n",
    "    fileData=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89c38f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(fileData), type(fileData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9054b440",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e260fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자열 => 문자열 리스트\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "data_list=sent_tokenize(fileData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971d08d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'data_list => {len(data_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2b8642",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e566bc1",
   "metadata": {},
   "source": [
    "##### [2] 토큰화 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eb247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileToken=Tokenizer()\n",
    "# raw_data용 단어사전 생성\n",
    "fileToken.fit_on_texts(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e9a374",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'word_index : { len( fileToken.word_index)}개\\n{ fileToken.word_index } ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2493b6f5",
   "metadata": {},
   "source": [
    "##### [3] 문장 수치화 & 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be47449",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqData=fileToken.texts_to_sequences(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d08728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(seqData[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5be5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_list[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
