{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d6513fc",
   "metadata": {},
   "source": [
    "# 텍스트 전처리\n",
    "---\n",
    "- 패키지 설치\n",
    "    * NLTK : !pip install nltk (for 영어 전처리 => 한글 처리 불가능)  \n",
    "    (정제 - 형태소 나누기 - 각각 넘버링 => 이런 작업을 해주는 툴)\n",
    "    * KoNLPy : !pip install konlpy (for 한글 처리 가능)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7f3cfa",
   "metadata": {},
   "source": [
    "## [1] 토큰화(Tokenization)\n",
    "---\n",
    "- 문장/문서를 뜻이 있는 가장 작은 단위로 나눈 것 (like 형태소)\n",
    "- 나누어진 단어를 토큰(Token)이라고 함\n",
    "- 종류\n",
    "    * 문장 토큰화\n",
    "    * 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77209904",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "# nltk.download('all') # quiet=True)\n",
    "\n",
    "# 불용어 패키지 다운로드 (all로 했기 때문에 다 깔려 있긴함)\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68ab4d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text1 = \"Freeze!! Everybody guns down!!\"\n",
    "\n",
    "raw_text2 = 'Trained on billions of lines of code'\n",
    "\n",
    "# '''\n",
    "# ddd\n",
    "# '''    => 이렇게 넣으면 \\n ddd \\n 이런식으로 읽게 됨\n",
    "\n",
    "raw_text3 = 'To install the data.\\\n",
    "            first install NLTK (see https://www.nltk.org/install.html).\\\n",
    "            then use NLTK’s data downloader as described below.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d97a6061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Freeze', '!', '!', 'Everybody', 'guns', 'down', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "# 단어 단위 토큰화\n",
    "result1 = word_tokenize(raw_text1)\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a23c46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4de89cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Freeze!! Everybody guns down!!', 'Trained on billions of lines of code']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장 단위 토큰화\n",
    "raw_text = [raw_text1, raw_text2]\n",
    "raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4c85e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaaa']\n"
     ]
    }
   ],
   "source": [
    "sent_result = sent_tokenize(\"aaaa\")\n",
    "print(sent_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11f7356d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Freeze!!', 'Everybody guns down!', '!'] 3\n"
     ]
    }
   ],
   "source": [
    "# 마침표를 하나의 문장으로 봐줌\n",
    "result = sent_tokenize(raw_text1)\n",
    "print(result, len(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196b4513",
   "metadata": {},
   "source": [
    "### 여러 문장의 토큰 추출\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bad792d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F']\n",
      "['r']\n",
      "['e']\n",
      "['e']\n",
      "['z']\n",
      "['e']\n",
      "['!']\n",
      "['!']\n",
      "[]\n",
      "['E']\n",
      "['v']\n",
      "['e']\n",
      "['r']\n",
      "['y']\n",
      "['b']\n",
      "['o']\n",
      "['d']\n",
      "['y']\n",
      "[]\n",
      "['g']\n",
      "['u']\n",
      "['n']\n",
      "['s']\n",
      "[]\n",
      "['d']\n",
      "['o']\n",
      "['w']\n",
      "['n']\n",
      "['!']\n",
      "['!']\n"
     ]
    }
   ],
   "source": [
    "# 문장 단위로 추출 (분리된 문장)\n",
    "for sent in raw_text1:    # 해당 text에서 문장을 하나씩 빼\n",
    "    total_token = []\n",
    "\n",
    "    # 문장 추출\n",
    "    sentResult = sent_tokenize(sent)\n",
    "    print(sentResult)\n",
    "    \n",
    "#     # 문장에서 추출한 토큰\n",
    "#     print(f' sent => {sent}')\n",
    "#     sentToken = word_tokenize(sent)\n",
    "#     print(f' sentToken => {sentToken}')\n",
    "    \n",
    "#     # 모든 문장의 토큰에 추가\n",
    "#     total_token.append(sentToken)\n",
    "\n",
    "# print('---')\n",
    "# print(total_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c3937b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Freeze!! Everybody guns down!!', 'Trained on billions of lines of code']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장 단위 토큰화\n",
    "raw_text = [raw_text1, raw_text2]\n",
    "raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2306d0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ele => Freeze!!\n",
      " ele => ['Freeze', '!', '!']\n",
      "\n",
      " ele => Everybody guns down!\n",
      " ele => ['Everybody', 'guns', 'down', '!']\n",
      "\n",
      " ele => !\n",
      " ele => ['!']\n",
      "\n",
      " ele => Freeze!!\n",
      " ele => ['Freeze!', '!']\n",
      "\n",
      " ele => Everybody guns down!\n",
      " ele => ['Everybody guns down!']\n",
      "\n",
      " ele => !\n",
      " ele => ['!']\n",
      "\n",
      " ele => Trained on billions of lines of code\n",
      " ele => ['Trained', 'on', 'billions', 'of', 'lines', 'of', 'code']\n",
      "\n",
      " ele => Trained on billions of lines of code\n",
      " ele => ['Trained on billions of lines of code']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 문장 단위로 추출 (문장이 합쳐진 상태)\n",
    "\n",
    "for sent in raw_text:    # 해당 text에서 문장을 하나씩 빼\n",
    "    total_token = []\n",
    "#     print(sent)\n",
    "    \n",
    "    # 문장 단위로 추출해서 토큰화 (리스트에 담아줌)\n",
    "    sentResult = sent_tokenize(sent)\n",
    "    \n",
    "    for ele in sentResult:\n",
    "        print(f' ele => {ele}')\n",
    "        wordResult = word_tokenize(ele)\n",
    "        print(f' ele => {wordResult}\\n')\n",
    "\n",
    "    for ele in sentResult:\n",
    "        print(f' ele => {ele}')\n",
    "        wordResult = sent_tokenize(ele)\n",
    "        print(f' ele => {wordResult}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cb92b2",
   "metadata": {},
   "source": [
    "- 토탈 토큰을 set()으로 두면 중복 제거\n",
    "- 중복 체크를 하든지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d9f9a1",
   "metadata": {},
   "source": [
    "# 문장 단위로 추출\n",
    "sl = [s, s2, s3]\n",
    "total_token=[]\n",
    "\n",
    "for sent in sl:\n",
    "    # 문장 추출\n",
    "    sent_result = sent_tokenize(sent)\n",
    "    \n",
    "    # 문장에서 추출한 토큰\n",
    "    for ele in sent_result:\n",
    "\n",
    "        word_token = word_tokenize(sent)\n",
    "        \n",
    "    # 모든 문장 토큰에 추가\n",
    "        total_token.append(word_token)\n",
    "        \n",
    "print(total_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba184c7",
   "metadata": {},
   "source": [
    "### 한글\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e63bf7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "# 형태소 분리 객체\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca75a00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오늘', '은', '월요일', '입니다', '.']\n"
     ]
    }
   ],
   "source": [
    "result = okt.morphs(\"오늘은 월요일입니다.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb86a3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('오늘', 'Noun'), ('은', 'Josa'), ('월요일', 'Noun'), ('입니다', 'Adjective'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "# 형태소 분리 후, 태깅(Tagging) => 품사\n",
    "result2 = okt.pos(\"오늘은 월요일입니다.\")\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4dca45c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('오늘', 'Noun'), ('은', 'Josa'), ('월요일', 'Noun'), ('이다', 'Adjective'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "# 형태소 분리 후, 태깅(Tagging) => 품사\n",
    "result2 = okt.pos(\"오늘은 월요일입니다.\", stem = True)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38c0c2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem=True를 주면 어간이 나옴 => [ 입니다 -> 이다 ]로 바뀜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8082ec",
   "metadata": {},
   "source": [
    "### [2] 정제 & 정규화\n",
    "---\n",
    "- 불용어 제거 => 노이즈 제거\n",
    "- 텍스트 동일화\n",
    "    * 대문자 or 소문자로 통일\n",
    "    * 문장의 길이"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361d37eb",
   "metadata": {},
   "source": [
    "### [2-1] 불용어 (Stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b384b30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# 영어의 불용어로 들어가 있는 것\n",
    "# -> 먼저 제거해도 아무런 영향이 없다는 의미\n",
    "# -> 불용어가 없으면 만들어줘야겠지\n",
    "\n",
    "en_stopwords = nltk.corpus.stopwords.words('english')\n",
    "print(len(en_stopwords))\n",
    "print(en_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058c14e7",
   "metadata": {},
   "source": [
    "### [2-2] 어간 및 표제어 처리\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7489c30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어간 추출\n",
    "\n",
    "# 형태만 보고 잘라줌\n",
    "# -es나 y->i로 고쳐지는 것들도 있는데, 원형 찾아주지 않고 그대로 잘라버림\n",
    "\n",
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f49f5914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "happy happy happy\n",
      "amus amus amus\n"
     ]
    }
   ],
   "source": [
    "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
    "print(stemmer.stem('happy'), stemmer.stem('happiness'), stemmer.stem('happier'))\n",
    "print(stemmer.stem('amuse'), stemmer.stem('amused'), stemmer.stem('amusing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a6e9bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표제어 (사전에 등록된 단어 추출)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64057806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working working work\n",
      "work working worked\n"
     ]
    }
   ],
   "source": [
    "# 품사를 적어줘야 함 (명확하게 추출하고 싶다면)\n",
    "print(lemmatizer.lemmatize('working'), lemmatizer.lemmatize('working', 'n'), lemmatizer.lemmatize('worked', 'v'))\n",
    "\n",
    "# 마지막에 'v'와 'n'의 차이 볼 것\n",
    "print(lemmatizer.lemmatize('working', 'v'), lemmatizer.lemmatize('working', 'n'), lemmatizer.lemmatize('worked', 'n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "69bbd5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work working work\n",
      "amuse amuse amusing\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize('working', 'v'), lemmatizer.lemmatize('working', 'n'), lemmatizer.lemmatize('worked', 'v'))\n",
    "print(lemmatizer.lemmatize('amusing', 'v'), lemmatizer.lemmatize('amused', 'v'), lemmatizer.lemmatize('amusing', 'n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145bd785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "149cd8c3",
   "metadata": {},
   "source": [
    "### [3] 텍스트 벡터화\n",
    "---\n",
    "- 텍스트 => 수치화\n",
    "- 희소벡터(OHE) : BOW 방식 => Count 기반, TF-IDF 기반\n",
    "- 밀집벡터 : Embedding 방식, Word2Vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "8da978ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "c4778ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpos = [raw_text1, raw_text2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "242a2f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "562aece3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 7)\t2\n",
      "  (1, 8)\t1\n",
      "  (1, 9)\t1\n"
     ]
    }
   ],
   "source": [
    "ohe.fit(corpos)\n",
    "ret = ohe.transform(corpos)\n",
    "print(type(ret), ret, sep='\\n')\n",
    "# (x,y) => x행의 y열 자리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "0414804e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 10)\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 7)\t2\n",
      "  (1, 8)\t1\n",
      "  (1, 9)\t1\n"
     ]
    }
   ],
   "source": [
    "# OHE한 것으로 볼 수 있음\n",
    "# ret = ret.toarray()\n",
    "print(ret.shape, ret, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "acefe646",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 전체 문서에서 나온 횟수 / 전체 문서\n",
    "# DF-IDF 기반\n",
    "tfIdf = TfidfVectorizer()\n",
    "tf_corpus = tfIdf.fit_transform(corpos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "0d6a7400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 2)\t0.5\n",
      "  (0, 5)\t0.5\n",
      "  (0, 3)\t0.5\n",
      "  (0, 4)\t0.5\n",
      "  (1, 1)\t0.33333333333333337\n",
      "  (1, 6)\t0.33333333333333337\n",
      "  (1, 7)\t0.6666666666666667\n",
      "  (1, 0)\t0.33333333333333337\n",
      "  (1, 8)\t0.33333333333333337\n",
      "  (1, 9)\t0.33333333333333337\n"
     ]
    }
   ],
   "source": [
    "print(type(tf_corpus), tf_corpus, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "c438eb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 10)\n",
      "  (0, 2)\t0.5\n",
      "  (0, 5)\t0.5\n",
      "  (0, 3)\t0.5\n",
      "  (0, 4)\t0.5\n",
      "  (1, 1)\t0.33333333333333337\n",
      "  (1, 6)\t0.33333333333333337\n",
      "  (1, 7)\t0.6666666666666667\n",
      "  (1, 0)\t0.33333333333333337\n",
      "  (1, 8)\t0.33333333333333337\n",
      "  (1, 9)\t0.33333333333333337\n"
     ]
    }
   ],
   "source": [
    "# tf_corpus = tf_corpus.toarray()\n",
    "print(tf_corpus.shape, tf_corpus, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "270f8ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "ccc16366",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent='Wiki is in Ward is original description: The simplest online database that could possibly work.\\\n",
    "Wiki is a piece of server software that allows users to freely create and edit Web page content using any Web browser. Wiki supports hyperlinks and has a simple text syntax for creating new pages and crosslinks between internal pages on the fly.\\\n",
    "Wiki is unusual among group communication mechanisms in that it allows the organization of contributions to be edited in addition to the content itself.Like many simple concepts, \"open editing\" has some profound and subtle effects on Wiki usage. Allowing everyday users to create and edit any page in a Web site is exciting in that it encourages democratic use of the Web and promotes content composition by nontechnical users.'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bb3fa6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "109e131d",
   "metadata": {},
   "source": [
    "## Tokenizer 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "e7f5211f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "# Tokenizer는 문장으로부터 단어를 토큰화하고 숫자에 대응시키는 딕셔너리를 사용할 수 있도록 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "6faf91f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text='Wiki is in Ward is original description: The simplest online database that could possibly work.\\\n",
    "Wiki is a piece of server software that allows users to freely create and edit Web page content using any Web browser. Wiki supports hyperlinks and has a simple text syntax for creating new pages and crosslinks between internal pages on the fly.\\\n",
    "Wiki is unusual among group communication mechanisms in that it allows the organization of contributions to be edited in addition to the content itself.Like many simple concepts, \"open editing\" has some profound and subtle effects on Wiki usage. Allowing everyday users to create and edit any page in a Web site is exciting in that it encourages democratic use of the Web and promotes content composition by nontechnical users.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "8f1c707d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 ['wiki', 'is', 'in', 'ward', 'is', 'original', 'description', 'the', 'simplest', 'online', 'database', 'that', 'could', 'possibly', 'work', 'wiki', 'is', 'a', 'piece', 'of', 'server', 'software', 'that', 'allows', 'users', 'to', 'freely', 'create', 'and', 'edit', 'web', 'page', 'content', 'using', 'any', 'web', 'browser', 'wiki', 'supports', 'hyperlinks', 'and', 'has', 'a', 'simple', 'text', 'syntax', 'for', 'creating', 'new', 'pages', 'and', 'crosslinks', 'between', 'internal', 'pages', 'on', 'the', 'fly', 'wiki', 'is', 'unusual', 'among', 'group', 'communication', 'mechanisms', 'in', 'that', 'it', 'allows', 'the', 'organization', 'of', 'contributions', 'to', 'be', 'edited', 'in', 'addition', 'to', 'the', 'content', 'itself', 'like', 'many', 'simple', 'concepts', 'open', 'editing', 'has', 'some', 'profound', 'and', 'subtle', 'effects', 'on', 'wiki', 'usage', 'allowing', 'everyday', 'users', 'to', 'create', 'and', 'edit', 'any', 'page', 'in', 'a', 'web', 'site', 'is', 'exciting', 'in', 'that', 'it', 'encourages', 'democratic', 'use', 'of', 'the', 'web', 'and', 'promotes', 'content', 'composition', 'by', 'nontechnical', 'users']\n"
     ]
    }
   ],
   "source": [
    "# 토큰으로 나누기\n",
    "tokens = text_to_word_sequence(raw_text)\n",
    "print(len(tokens), tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "72e74711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수 인코딩\n",
    "myToken = Tokenizer(num_words=10)\n",
    "# 만든 voca들 중의 인덱스를 뽑을 때의 최대범위ㅏ\n",
    "# - 입력하지 않으면 전체 범위\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 단어 빈도에 따른 사용할 단어 개수의 최대값. 가장 빈번하게 사용되는 num_words개의 단어만 보존합니다.\n",
    "# -> 빈도 높은 것 부터 내림차순, 그 빈도 숫자까지만 쓰겠다\n",
    "# ----------------------------------------------------------------\n",
    "# [ num_words=10 ] 이기 때문에 9번이 뜨는 것\n",
    "# [ num_words=5 ] 라면 뜨지 않음 => OOV(out of voca) 가 됨\n",
    "# voca에서는 번호를 매길 때 1번부터 매김 (패딩을 0으로 줘야하기 때문)\n",
    "\n",
    "# 0을 쓰고자 한다면\n",
    "# i) [ oov_token = 1 ] 로 두면 인덱스 번호가 2번부터 시작하게끔 밀림\n",
    "# ii) [ 총 인덱스 개수 + 1 ] 로 해서 \n",
    "# => but, 대부분 패딩을 '0'으로 쓰기 때문에 \n",
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8397ec2",
   "metadata": {},
   "source": [
    "num_words: 단어 빈도에 따른 사용할 단어 개수의 최대값. 가장 빈번하게 사용되는 num_words개의 단어만 보존합니다.\n",
    "filters: 문자열로, 각 성분이 텍스트에서 걸러진 문자에 해당됩니다. 디폴트 값은 모든 구두점이며, 거기에 탭과 줄 바꿈은 추가하고 ' 문자는 제외합니다.\n",
    "lower: 불리언. 텍스트를 소문자로 변환할지 여부.\n",
    "split: 문자열. 단어 분해 용도의 분리기.\n",
    "char_level: 참인 경우 모든 문자가 토큰으로 처리됩니다.\n",
    "oov_token: 값이 지정된 경우, text_to_sequence 호출 과정에서 단어색인(word_index)에 추가되어 어휘목록 외 단어를 대체합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "88f11a36",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 1, 'wiki': 2, 'is': 3, 'in': 4, 'the': 5, 'that': 6, 'to': 7, 'web': 8, 'a': 9, 'of': 10, 'users': 11, 'content': 12, 'allows': 13, 'create': 14, 'edit': 15, 'page': 16, 'any': 17, 'has': 18, 'simple': 19, 'pages': 20, 'on': 21, 'it': 22, 'ward': 23, 'original': 24, 'description': 25, 'simplest': 26, 'online': 27, 'database': 28, 'could': 29, 'possibly': 30, 'work': 31, 'piece': 32, 'server': 33, 'software': 34, 'freely': 35, 'using': 36, 'browser': 37, 'supports': 38, 'hyperlinks': 39, 'text': 40, 'syntax': 41, 'for': 42, 'creating': 43, 'new': 44, 'crosslinks': 45, 'between': 46, 'internal': 47, 'fly': 48, 'unusual': 49, 'among': 50, 'group': 51, 'communication': 52, 'mechanisms': 53, 'organization': 54, 'contributions': 55, 'be': 56, 'edited': 57, 'addition': 58, 'itself': 59, 'like': 60, 'many': 61, 'concepts': 62, 'open': 63, 'editing': 64, 'some': 65, 'profound': 66, 'subtle': 67, 'effects': 68, 'usage': 69, 'allowing': 70, 'everyday': 71, 'site': 72, 'exciting': 73, 'encourages': 74, 'democratic': 75, 'use': 76, 'promotes': 77, 'composition': 78, 'by': 79, 'nontechnical': 80}\n"
     ]
    }
   ],
   "source": [
    "myToken.fit_on_texts(tokens)\n",
    "# fit_on_texts() : 문자 데이터를 입력받아서 리스트의 형태로 변환\n",
    "print(myToken.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "08a4a182",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('wiki', 5), ('is', 5), ('in', 5), ('ward', 1), ('original', 1), ('description', 1), ('the', 5), ('simplest', 1), ('online', 1), ('database', 1), ('that', 4), ('could', 1), ('possibly', 1), ('work', 1), ('a', 3), ('piece', 1), ('of', 3), ('server', 1), ('software', 1), ('allows', 2), ('users', 3), ('to', 4), ('freely', 1), ('create', 2), ('and', 6), ('edit', 2), ('web', 4), ('page', 2), ('content', 3), ('using', 1), ('any', 2), ('browser', 1), ('supports', 1), ('hyperlinks', 1), ('has', 2), ('simple', 2), ('text', 1), ('syntax', 1), ('for', 1), ('creating', 1), ('new', 1), ('pages', 2), ('crosslinks', 1), ('between', 1), ('internal', 1), ('on', 2), ('fly', 1), ('unusual', 1), ('among', 1), ('group', 1), ('communication', 1), ('mechanisms', 1), ('it', 2), ('organization', 1), ('contributions', 1), ('be', 1), ('edited', 1), ('addition', 1), ('itself', 1), ('like', 1), ('many', 1), ('concepts', 1), ('open', 1), ('editing', 1), ('some', 1), ('profound', 1), ('subtle', 1), ('effects', 1), ('usage', 1), ('allowing', 1), ('everyday', 1), ('site', 1), ('exciting', 1), ('encourages', 1), ('democratic', 1), ('use', 1), ('promotes', 1), ('composition', 1), ('by', 1), ('nontechnical', 1)])\n"
     ]
    }
   ],
   "source": [
    "print(myToken.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "48af6b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(myToken.word_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "41dac7a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[9]]"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myToken.texts_to_sequences('a')\n",
    "# ()안의 text값을 인덱스 값으로 반환\n",
    "\n",
    "# myToken = Tokenizer(num_words=10)\n",
    "# [ num_words=10 ] 이기 때문에 9번이 뜨는 것\n",
    "# [ num_words=5 ] 라면 뜨지 않음 => OOV(out of voca) 가 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "a2e27404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 읽고, 토큰화(Tokenizer) 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934f9059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58001e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f38164d",
   "metadata": {},
   "source": [
    "### **Tokenizer 객체 생성-------------------------------**\n",
    "- 제공한 문서/문장에 대한 단어사전(voca)\n",
    "- 단어사전(voca)에 존재하지 않는 단어 => Out Of Voca(OOV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "9b98fc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "  'I love my dog',\n",
    "  'I love my cat',\n",
    "  'You love my dog!',\n",
    "  'Do you think my dog is amazing?'\n",
    "]\n",
    "\n",
    "# {'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, \n",
    "#  'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n",
    "# ===> 느낌표, 마침표와 같은 구두점은 인코딩에 영향을 주지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "631081a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "# tokenizer = Tokenizer(num_words=3, oov_token=1)\n",
    "\n",
    "# 단어 빈도수가 높은 순으로 낮은 점수 인덱스 부여\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "# 공백기준으로 쪼개고, 소문자로 통일 해주는 과정 수행~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "6f221732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n",
      "\n",
      "OrderedDict([('i', 2), ('love', 3), ('my', 4), ('dog', 3), ('cat', 1), ('you', 2), ('do', 1), ('think', 1), ('is', 1), ('amazing', 1)])\n",
      "\n",
      "=> 둘의 순서가 같지 않음\n"
     ]
    }
   ],
   "source": [
    "# 단어 인덱스\n",
    "# -> word_index 속성은 단어와 숫자의 키-값 쌍을 포함하는 딕셔너리를 반환\n",
    "# -> 단어 빈도수가 높은 순으로 낮은 점수 인덱스 부여\n",
    "\n",
    "print(tokenizer.word_index)\n",
    "\n",
    "print()\n",
    "\n",
    "# 단어 출력 개수\n",
    "# 중복을 제거하면서, 순서대로 [ 토큰 : 개수 ]를 나타냄\n",
    "print(tokenizer.word_counts)\n",
    "\n",
    "print('\\n=> 둘의 순서가 같지 않음')\n",
    "\n",
    "# [ oov_token = 1 ]\n",
    "# 1: 1이 생기면서, 다른 단어들이 다 밀림\n",
    "# => 이게 OOV로 들어간다는 것\n",
    "# => num_words에도 밀리니까 결국, 3일 때 1,2만 나오게 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "45e7d1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_voca : 4\n",
      "[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]\n"
     ]
    }
   ],
   "source": [
    "# texts_to_sequences() : 텍스트 안의 단어들을 숫자의 시퀀스의 형태로 변환\n",
    "# ==> 문장을 생성된 사전(voca)를 기반으로 수치화\n",
    "\n",
    "seq_voca = tokenizer.texts_to_sequences(sentences)\n",
    "print(f'seq_voca : {len(seq_voca)}')\n",
    "print(seq_voca)\n",
    "\n",
    "# [ num_words=3 ] 인데 3이 안나오네?\n",
    "# -> [ num_words=3+1 ]이면 나오지!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c89c4c8",
   "metadata": {},
   "source": [
    "# 패딩(Padding)\n",
    "---\n",
    "- 길이가 모두 다른 문장들을 동일 길이로 맞추기 위한 과정\n",
    "- 길이 기준 설정\n",
    "- 긴 경우 => 앞/뒤 중 선택\n",
    "- 짧은 경우 => 앞/뒤 중 선택\n",
    "- 값 => 패딩에 들어갈 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "d1e4b69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "b257cce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_=tokenizer.texts_to_sequences(sentences)\n",
    "result_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "711897e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  4,  2,  1,  3],\n",
       "       [ 0,  0,  0,  4,  2,  1,  6],\n",
       "       [ 0,  0,  0,  5,  2,  1,  3],\n",
       "       [ 7,  5,  8,  1,  3,  9, 10]])"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가장 긴 것에 길이를 맞춰주기 위해, '0'으로 채워진 것을 볼 수 있음\n",
    "encoding = pad_sequences(result_)\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718fef65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd41cdb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "23c7077f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 수치화한 것을 원-핫 인코딩 수행\n",
    "# sklearn OneHotEncoder 객체 생성 (이미 라벨 인코더는 된 셈)\n",
    "# keras 함수\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "to_categorical(seq_voca[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "f2c53424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_matrix(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e02907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "84e58145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 토큰화 ( 문장단위, 워드단위 => 상황에 맞게 )\n",
    "\n",
    "\n",
    "#  -> 인덱스 부여\n",
    "#  -> 원-핫 인코딩\n",
    "#  -> 임베딩 작업\n",
    "\n",
    "# 2. 정제(Cleaning) 및 정규화(Normalization)\n",
    "# - 불용어 제거 (뽑은 토큰에서 불용어 제거)\n",
    "# - 어간 -> 사전에 존재하지 않는 경우가 있을 수도 \n",
    "# - 표제어 -> 기본 사전에 존재하는 단어. 뿌리 단어를 찾아서 단어의 개수를 줄임\n",
    "\n",
    "# 3. 피처 백터화 => 수치화\n",
    "# - 단순 카운트 기반 벡터화\n",
    "# - TF (얼마나 자주 나오는가), DF (다른 문서에는 얼마나 나오는가)\n",
    "#  -> TF가 높고, DF가 낮아야 => 해당 문서에서 중요한 단어라는 것 (가중치 부여)\n",
    "#  -> 가중치를 매겨서 중요도를 보겠다는 것\n",
    "\n",
    "# 4. 패딩\n",
    "# - 가변 길이의 문장들을 동일 길이로 맞추어 주는 것\n",
    "# - 지정된 길이보다 길면 자르기 (기본은 앞을 자르기(뒤를 남기고))\n",
    "# - 지정된 길이보다 짧으면 0으로 채우기 (=> 단어 인덱스가 1부터 시작하는 이유)\n",
    "# - OHE하기 전에 패딩을 하도록\n",
    "\n",
    "# 5. 임배딩 (Word2Vec, FastText, BERT)\n",
    "# - 벡터 값들이 실수로 나오게 되고, 비슷하게 나오면 비슷한 것이라 보면 됨\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
